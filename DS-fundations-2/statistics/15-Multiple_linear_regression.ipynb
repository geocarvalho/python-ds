{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression introduction\n",
    "We can use linear algebra to predict the **$Y$** value using all multi-column at the same time. The way we do this is by creating a matrix of inputs, and we create a vector of the response that we want to predict. The matrix with the multiple column is denoted **$Y$** (capital x bold) while the vector for the response is denoted **$y$** (y bold)\n",
    "* [Linear algebra from Khan academy](https://www.khanacademy.org/math/linear-algebra)\n",
    "* [an introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/) - chapter 3\n",
    "\n",
    "In this notebook (and following quizzes), you will be creating a few simple linear regression models, as well as a multiple linear regression model, to predict home value.\n",
    "\n",
    "Let's get started by importing the necessary libraries and reading in the data you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm;\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./house_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:23</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:23   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "mlm = sm.OLS(df['price'], df[['area', 'bedrooms', 'bathrooms', 'intercept']])\n",
    "results = mlm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Using statsmodels, fit three individual simple linear regression models to predict price.  You should have a model that uses **area**, another using **bedrooms**, and a final one using **bathrooms**.  You will also want to use an intercept in each of your three models.\n",
    "\n",
    "Use the results from each of your models to answer the first two quiz questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.269e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:23</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 9587.8878</td> <td> 7637.479</td> <td>    1.255</td> <td> 0.209</td> <td>-5384.303</td> <td> 2.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  348.4664</td> <td>    3.093</td> <td>  112.662</td> <td> 0.000</td> <td>  342.403</td> <td>  354.530</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>368.609</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 349.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.534</td>  <th>  Prob(JB):          </th> <td>1.43e-76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.499</td>  <th>  Cond. No.          </th> <td>4.93e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.93e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                 1.269e+04\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:23   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6026   BIC:                         1.691e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   9587.8878   7637.479      1.255      0.209   -5384.303    2.46e+04\n",
       "area         348.4664      3.093    112.662      0.000     342.403     354.530\n",
       "==============================================================================\n",
       "Omnibus:                      368.609   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              349.279\n",
       "Skew:                           0.534   Prob(JB):                     1.43e-76\n",
       "Kurtosis:                       2.499   Cond. No.                     4.93e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.93e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "lm1 = sm.OLS(df['price'], df[['intercept', 'area']])\n",
    "result1 = lm1.fit()\n",
    "result1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7446.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:24</td>     <th>  Log-Likelihood:    </th> <td> -85509.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.710e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.710e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-9.485e+04</td> <td> 1.08e+04</td> <td>   -8.762</td> <td> 0.000</td> <td>-1.16e+05</td> <td>-7.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td> 2.284e+05</td> <td> 2646.744</td> <td>   86.289</td> <td> 0.000</td> <td> 2.23e+05</td> <td> 2.34e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>967.118</td> <th>  Durbin-Watson:     </th> <td>   2.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1599.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.074</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.325</td>  <th>  Cond. No.          </th> <td>    10.3</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.553\n",
       "Model:                            OLS   Adj. R-squared:                  0.553\n",
       "Method:                 Least Squares   F-statistic:                     7446.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:24   Log-Likelihood:                -85509.\n",
       "No. Observations:                6028   AIC:                         1.710e+05\n",
       "Df Residuals:                    6026   BIC:                         1.710e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept  -9.485e+04   1.08e+04     -8.762      0.000   -1.16e+05   -7.36e+04\n",
       "bedrooms    2.284e+05   2646.744     86.289      0.000    2.23e+05    2.34e+05\n",
       "==============================================================================\n",
       "Omnibus:                      967.118   Durbin-Watson:                   2.014\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1599.431\n",
       "Skew:                           1.074   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.325   Cond. No.                         10.3\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2 = sm.OLS(df['price'], df[['intercept', 'bedrooms']])\n",
    "result2 = lm2.fit()\n",
    "result2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.541</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.541</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7116.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:24</td>     <th>  Log-Likelihood:    </th> <td> -85583.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.712e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.712e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 4.314e+04</td> <td> 9587.189</td> <td>    4.500</td> <td> 0.000</td> <td> 2.43e+04</td> <td> 6.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 3.295e+05</td> <td> 3905.540</td> <td>   84.358</td> <td> 0.000</td> <td> 3.22e+05</td> <td> 3.37e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>915.429</td> <th>  Durbin-Watson:     </th> <td>   2.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1537.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.010</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.428</td>  <th>  Cond. No.          </th> <td>    5.84</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.541\n",
       "Model:                            OLS   Adj. R-squared:                  0.541\n",
       "Method:                 Least Squares   F-statistic:                     7116.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:24   Log-Likelihood:                -85583.\n",
       "No. Observations:                6028   AIC:                         1.712e+05\n",
       "Df Residuals:                    6026   BIC:                         1.712e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   4.314e+04   9587.189      4.500      0.000    2.43e+04    6.19e+04\n",
       "bathrooms   3.295e+05   3905.540     84.358      0.000    3.22e+05    3.37e+05\n",
       "==============================================================================\n",
       "Omnibus:                      915.429   Durbin-Watson:                   2.003\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1537.531\n",
       "Skew:                           1.010   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.428   Cond. No.                         5.84\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3 = sm.OLS(df['price'], df[['intercept', 'bathrooms']])\n",
    "result3 = lm3.fit()\n",
    "result3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have looked at the results from the simple linear regression models, let's try a multiple linear regression model using all three of these variables  at the same time.  You will still want an intercept in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:00:24</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:00:24   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms']])\n",
    "result = lm.fit()\n",
    "result.summary() # multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Along with using the **area**, **bedrooms**, and **bathrooms** you might also want to use **style** to predict the price.  Try adding this to your multiple linear regression model.  What happens?  Use the final quiz below to provide your answer.\n",
    "\n",
    "* There is an error, because an object cannot be added to the multiple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-af8693246f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstyle_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'area'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bedrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bathrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult_style\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m                  **kwargs):\n\u001b[1;32m    816\u001b[0m         super(OLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 817\u001b[0;31m                                   hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"weights\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         super(WLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 663\u001b[0;31m                                   weights=weights, hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \"\"\"\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pinv_wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wendog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m---> 64\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0;32m--> 633\u001b[0;31m                  **kwargs)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_endog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_exog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# this has side-effects, attaches k_constant and const_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/statsmodels/base/data.py\u001b[0m in \u001b[0;36m_convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             raise ValueError(\"Pandas data cast to numpy dtype of object. \"\n\u001b[0m\u001b[1;32m    475\u001b[0m                              \"Check input data with np.asarray(data).\")\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPandasData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "style_lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms', 'style']])\n",
    "result_style = style_lm.fit()\n",
    "result_style.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y matrix\n",
    "X = df[['intercept', 'area', 'bedrooms', 'bathrooms']]\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression closed form solution:\n",
    "\n",
    "$\\hat{\\beta}=(X`X)^-X`y$\n",
    "\n",
    "$X`$ means X transpose\n",
    "\n",
    "$()‚Åª$ means the inverstion of the results inside the parentheses\n",
    "\n",
    "In Numpy get the transpose, the inverses and the dot products to get the coeficients results for each column\n",
    "\n",
    "* [OLS in matrix form](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10072.10704672,   345.91101884, -2925.80632467,  7345.3917137 ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = np.linalg.inv(np.dot(X.transpose(),X)) #(X'X)^-\n",
    "beta = np.dot(beta, X.transpose()) #(X'X)^- * X'\n",
    "beta = np.dot(beta, y) # (X'X)^-X' * y\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables\n",
    "Instead of adding columns with categorical variables, create columns with the presence or not of a variable (been the column name the varible present or not) enconded by 1 (exist) and 0 (not exist). Because the last column can be inferred from the earlier two columns, as the one values are only in the rows that didn't have ones in one of these other columns, we actually end up choosing to drop this column (which one doesn't really matter too much, noted as reference column). Without the last column, the matrix is [full rank](https://www.cds.caltech.edu/~murray/amwiki/index.php/FAQ:_What_does_it_mean_for_a_non-square_matrix_to_be_full_rank%3F)\n",
    "![image-example](https://d17h27t6h515a5.cloudfront.net/topher/2017/December/5a297de8_screen-shot-2017-12-07-at-9.43.05-am/screen-shot-2017-12-07-at-9.43.05-am.png)\n",
    "\n",
    "* The number of variables dummy added to the matrix X and the level of each categorical variable minus one have to be equal;\n",
    "* The motivation to delete one dummy variable is to garanteee that all the column are linear independent, product of $X`X$ is invertible and the matrix X be full rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1548.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:01:00</td>     <th>  Log-Likelihood:    </th> <td> -86683.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6025</td>      <th>  BIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.046e+06</td> <td> 7775.607</td> <td>  134.534</td> <td> 0.000</td> <td> 1.03e+06</td> <td> 1.06e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lodge</th>     <td>-7.411e+05</td> <td> 1.44e+04</td> <td>  -51.396</td> <td> 0.000</td> <td>-7.69e+05</td> <td>-7.13e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td> -4.71e+05</td> <td> 1.27e+04</td> <td>  -37.115</td> <td> 0.000</td> <td>-4.96e+05</td> <td>-4.46e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1340.120</td> <th>  Durbin-Watson:     </th> <td>   2.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3232.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.230</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.611</td>  <th>  Cond. No.          </th> <td>    3.28</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.339\n",
       "Model:                            OLS   Adj. R-squared:                  0.339\n",
       "Method:                 Least Squares   F-statistic:                     1548.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:01:00   Log-Likelihood:                -86683.\n",
       "No. Observations:                6028   AIC:                         1.734e+05\n",
       "Df Residuals:                    6025   BIC:                         1.734e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.046e+06   7775.607    134.534      0.000    1.03e+06    1.06e+06\n",
       "lodge      -7.411e+05   1.44e+04    -51.396      0.000   -7.69e+05   -7.13e+05\n",
       "ranch       -4.71e+05   1.27e+04    -37.115      0.000   -4.96e+05   -4.46e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1340.120   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3232.810\n",
       "Skew:                           1.230   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.611   Cond. No.                         3.28\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['A', 'B', 'C']] = pd.get_dummies(df['neighborhood']) \n",
    "df[['lodge', 'ranch', 'victorian']] = pd.get_dummies(df['style'])\n",
    "# You always drop one level from each category. This is called the baseline (the dropped column)\n",
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'lodge', 'ranch']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The intercept means that if our home is a **victorian** home, we predict its price to be 1,046,000 dollars and **lodge** is predicted to be 741,000 less than a **victorian**. A **ranch** is predicted to be 471,000 less than a **victorian**.\n",
    "\n",
    "\n",
    "`1.` Use the [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) documentation to assist you with obtaining dummy variables for the **neighborhood** column.  Then use [join](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) to add the dummy variables to your dataframe, **df**, and store the joined results in **df_new**.\n",
    "\n",
    "Fit a linear model using **all three levels** of **neighborhood** to predict the price. Don't forget an intercept.\n",
    "\n",
    "Use your results to answer quiz 1 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['A', 'B', 'C'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0aa0c279160c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mneighborhood_dummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neighborhood'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighborhood_dummies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6324\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6325\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6326\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6328\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6339\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6340\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6341\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6342\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     59\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                          validate=validate)\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 573\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m             raise ValueError('columns overlap but no suffix specified: '\n\u001b[0;32m-> 5244\u001b[0;31m                              '{rename}'.format(rename=to_rename))\n\u001b[0m\u001b[1;32m   5245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5246\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['A', 'B', 'C'], dtype='object')"
     ]
    }
   ],
   "source": [
    "neighborhood_dummies = pd.get_dummies(df['neighborhood'])\n",
    "df_new = df.join(neighborhood_dummies)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-979d17b2ed0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "df_new['intercept'] = 1\n",
    "lm = sm.OLS(df_new['price'], df_new[['intercept', 'A', 'B', 'C']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.`  Now, fit an appropriate linear model for using **neighborhood** to predict the price of a home. Use **neighborhood A** as your baseline.  Use your resulting model to answer the questions in Quiz 2 and Quiz 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = sm.OLS(df_new['price'], df_new[['intercept', 'C', 'B']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observe que cada um dos coeficientes √© uma compara√ß√£o da categoria com a refer√™ncia. Portanto, um coeficiente positivo sugere que o bairro √© mais caro, em m√©dia, do que a refer√™ncia. Por outro lado, um coeficiente negativo sugere que o bairro √© menos caro, em m√©dia, do que a refer√™ncia.\n",
    "\n",
    "* Voc√™ pode olhar para os valores-p para comparar com o bairro A. Para comparar o bairro B ao bairro C, voc√™ pode comparar os intervalos de confian√ßa. Como os intervalos de confian√ßa para B e C n√£o se sobrep√µem, temos provas de que eles diferem tamb√©m.\n",
    "\n",
    "`3.` Run the two cells below to look at the home prices for the A and C neighborhoods. Add neighborhood B. This creates a glimpse into the differences that you found in the previous linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_new.query(\"C == 1\")['price'], alpha = 0.3, label = 'C');\n",
    "plt.hist(df_new.query(\"A == 1\")['price'], alpha = 0.3, label = 'A');\n",
    "plt.hist(df_new.query(\"B == 1\")['price'], alpha = 0.3, label = 'B')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now, add dummy variables for the **style** of house. Create a new linear model using these new dummies, as well as the previous **neighborhood** dummies.  Use **ranch** as the baseline for the **style**.  Additionally, add **bathrooms** and **bedrooms** to your linear model.  Don't forget an intercept.  Use the results of your linear model to answer the last two questions below. **Home prices are measured in dollars, and this dataset is not real.**\n",
    "\n",
    "To minimize scrolling, it might be useful to open another browser window to this concept to answer the quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sty = pd.get_dummies(df_new['style'])\n",
    "new_new_df = df_new.join(sty)\n",
    "lm = sm.OLS(new_new_df['price'], new_new_df[['intercept', 'B', 'C', 'lodge', 'victorian', 'bedrooms', 'bathrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 80.9%  (r-squared) da variabilidade no pre√ßo pode ser explicada pelo modelo linear constru√≠do usando o estilo de quartos, banheiros, vizinhan√ßa e da casa;\n",
    "* Para cada quarto adicional em uma casa, espera-se um aumento de pre√ßo de 173200, em que todas as outras vari√°veis s√£o mantidas constantes;\n",
    "* Para cada banheiro adicional em uma casa, espera-se um aumento de pre√ßo de 99960, em que todas as outras vari√°veis s√£o mantidas constantes;\n",
    "* Espera-se que uma casa vitoriana custar√° 70560 a mais do que uma fazenda, sendo todo o resto igual;\n",
    "* Espera-se que uma casa na vizinhan√ßa C custar√° 7168 menos que uma casa na vizinhan√ßa A, sendo todo o resto igual.\n",
    "\n",
    "---\n",
    "\n",
    "## Dummy variables recap\n",
    "\n",
    "The biggest reason for use encoding one, zero and negative one is that it changes the coefficients that we get back from the model as well as how interpret those coefficients.\n",
    "With one-zero encoding your interpreter coefficients as a comparion to a baseline category. However, if use one-zer-negative one encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df2 = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The below function creates 1, 0, -1 coded dummy variables.\n",
    "\n",
    "def dummy_cat(df, col):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - the dataframe where col is stored\n",
    "    col - the categorical column you want to dummy (as a string)\n",
    "    OUTPUT:\n",
    "    df - the dataframe with the added columns\n",
    "         for dummy variables using 1, 0, -1 coding\n",
    "    '''\n",
    "    for idx, val_0 in enumerate(df[col].unique()):\n",
    "        if idx + 1 < df[col].nunique():            \n",
    "            df[val_0] = df[col].apply(lambda x: 1 if x == val_0 else 0)\n",
    "        else:    \n",
    "            df[val_0] = df[col].apply(lambda x: -1 if x == val_0 else 0)\n",
    "            for idx, val_1 in enumerate(df[col].unique()):\n",
    "                if idx + 1 < df[col].nunique():\n",
    "                    df[val_1] = df[val_0] + df[val_1]\n",
    "                else:\n",
    "                    del df[val_1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = dummy_cat(df, 'style') # Use on style\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['intercept'] = 1\n",
    "\n",
    "lm = sm.OLS(new_df['price'], new_df[['intercept', 'ranch', 'victorian']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dummies = pd.get_dummies(df['style'])\n",
    "new_df2 = df2.join(style_dummies)\n",
    "new_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2['intercept'] = 1\n",
    "\n",
    "lm2 = sm.OLS(new_df2['price'], new_df2[['intercept', 'ranch', 'victorian']])\n",
    "results2 = lm2.fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns pontos a serem observados: Em primeiro lugar, a programa√ß√£o 1, 0 significa uma compara√ß√£o com a categoria de base. Depois, a programa√ß√£o 1, 0,-1 significa uma compara√ß√£o com a m√©dia geral. Por fim, a linguagem de aumento de uma unidade est√° associada √†s vari√°veis quantitativas, e n√£o √†s vari√°veis categ√≥ricas.\n",
    "\n",
    "* 33.9% da variabilidade no pre√ßo pode ser explicada pelo estilo de casa.\n",
    "* 642100 √© o pre√ßo m√©dio de moradia previsto, sem levar em conta o estilo.\n",
    "* Em compara√ß√£o a uma hospedaria, prevemos que uma casa vitoriana tenha uma alta de pre√ßo de 741100, mantendo todo o resto constante.\n",
    "* Em compara√ß√£o a uma casa mediana, prevemos que o pre√ßo de uma casa vitoriana seja 404000 maior, mantendo todas as outras vari√°veis constantes.\n",
    "\n",
    "Para prever a categoria de refer√™ncia na codifica√ß√£o 1, 0, voc√™ tem que utilizar o intercepto. Na codifica√ß√£o 1, 0, -1, voc√™ precisa multiplicar cada coeficiente categ√≥rico por -1 para chegar na categoria que falta. Com isto em mente, qual √© o pre√ßo m√©dio previsto para hospedarias utilizando o modelo de codifica√ß√£o 1, 0, -1?\n",
    "\n",
    "* Multiplicando -1 pela fazenda e pela casa vitoriana, obtemos o seguinte resultado: 642100 + 66950 - 404000 = 305050. Observe tamb√©m que isso coincide com a mesma previs√£o (erro de 50 devido aos arredondamentos), que voc√™ v√™ no modelo de codifica√ß√£o 0,1\n",
    "\n",
    "---\n",
    "\n",
    "## Potential problems introduction\n",
    "\n",
    "There's a number of problems that may arise. First, what is your model for?\n",
    "* To understand if your X and Y variables are related?\n",
    "* To best predict the response variable?\n",
    "* Find which variables are really useful in predicting your response?\n",
    "\n",
    "Depending on which aspects you're most interesed in, this can help determine which problems you actually care about addressing.\n",
    "\n",
    "* A linear relationship may not exist between your response and predictor variables;\n",
    "* You might have correlated errors;\n",
    "* You might not have constant variance of your errors;\n",
    "* You might have outliers or leverage points that hurt your model;\n",
    "* You might have multicolliearity.\n",
    "\n",
    "Chapter 3 of \"[An introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)\" dives into each of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. N√£o-linearidade das rela√ß√µes entre preditor e resposta\n",
    "2. Correla√ß√£o dos termos de erro\n",
    "3. Vari√¢ncia n√£o-constante e erros normalmente distribu√≠dos\n",
    "4. Outliers/pontos de alta alavancagem\n",
    "5. Colinearidade\n",
    "\n",
    "### Linearidade\n",
    "A suposi√ß√£o de linearidade √© que o modelo linear representa a verdadeira rela√ß√£o existente entre a vari√°vel de resposta e a preditora. Se isso n√£o for verdade, ent√£o as suas previs√µes n√£o ser√£o muito precisas. Al√©m disso, as rela√ß√µes lineares associadas aos seus coeficientes tamb√©m n√£o s√£o muito √∫teis.\n",
    "\n",
    "Para avaliar se uma rela√ß√£o linear √© razo√°vel, um gr√°fico dos res√≠duos $(y-\\hat{y})$ pelos valores preditos (\\hat{y}) geralmente √© √∫til. Se existem padr√µes de curvatura neste gr√°fico, isso sugere que um modelo linear pode n√£o se ajustar adequadamente aos dados, e alguma outra rela√ß√£o existe entre as vari√°veis preditoras e de resposta. Existem muitas maneiras de criar modelos n√£o-lineares (at√© mesmo usando o formato do modelo linear), e voc√™ ser√° apresentado a algumas delas.\n",
    "\n",
    "Na imagem na parte inferior desta p√°gina, os modelos s√£o considerados viesados. O ideal seria que tiv√©ssemos uma dispers√£o aleat√≥ria de pontos como na figura do gr√°fico de res√≠duos do canto superior esquerdo.\n",
    "\n",
    "#### Erros correlacionados\n",
    "Os erros correlacionados frequentemente ocorrem quando nossos dados s√£o coletados ao longo do tempo (como na proje√ß√£o de pre√ßos de a√ß√µes ou taxas de juros futuras) ou quando os dados s√£o relacionados espacialmente (como a previs√£o de regi√µes de inunda√ß√µes ou secas). Muitas vezes podemos melhorar nossas previs√µes usando informa√ß√µes dos √∫ltimos pontos de dados (para o tempo) ou os pontos nas proximidades (para o espa√ßo).\n",
    "\n",
    "O principal problema em n√£o levar em conta os erros correlacionados √© que voc√™ poderia utilizar essa correla√ß√£o para sua vantagem, prevendo de uma maneira melhor os eventos futuros ou eventos espacialmente pr√≥ximos uns dos outros.\n",
    "\n",
    "Um dos jeitos mais comuns de identificar se os erros s√£o correlacionados √© verificar o dom√≠nio de onde os dados s√£o coletados. Se voc√™ n√£o tiver certeza, h√° um teste conhecido como teste [Durbin-Watson](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) que √© comumente usado para avaliar se a correla√ß√£o dos erros √© um problema. Depois disso, os modelos [ARIMA ou ARMA](http://www.statsref.com/HTML/index.html?arima.html) s√£o comumente implementados para usar esta correla√ß√£o em previs√µes melhores.\n",
    "\n",
    "#### Vari√¢ncia n√£o-constante e erros normalmente distribu√≠dos\n",
    "Vari√¢ncia n√£o-constante ocorre quando a propaga√ß√£o dos valores previstos difere dependendo de qual √© o valor que se est√° tentando prever. Isto n√£o √© um problema enorme em termos de uma boa previs√£o. No entanto, isso leva a intervalos de confian√ßa e p-valores que s√£o imprecisos. Os intervalos de confian√ßa para os coeficientes ser√£o amplos demais para √°reas onde os valores reais est√£o mais perto dos valores previstos, mas ser√£o muito estreitos para √°reas onde os valores reais est√£o mais separados dos valores previstos.\n",
    "\n",
    "Comumente, um logaritmo (ou alguma outra transforma√ß√£o da vari√°vel de resposta) √© feita para ‚Äúse livrar‚Äù da vari√¢ncia n√£o-constante. A fim de escolher a transforma√ß√£o, um [Box-Cox](http://www.statisticshowto.com/box-cox-transformation/) √© geralmente usado.\n",
    "\n",
    "A vari√¢ncia n√£o constante pode ser avaliada novamente, usando um gr√°fico dos res√≠duos pelos valores previstos. Na imagem na parte inferior da p√°gina, a vari√¢ncia n√£o-constante √© rotulada como **heterosced√°stica**. Idealmente, queremos um modelo n√£o viesado e com res√≠duos homoced√°sticos (consistente em toda o intervalo de valores).\n",
    "\n",
    "Embora o texto n√£o aborde a normalidade dos res√≠duos, esta √© uma suposi√ß√£o importante da regress√£o se voc√™ est√° interessado em criar intervalos de confian√ßa adequados. Mais sobre este tema √© fornecido [aqui](http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm).\n",
    "\n",
    "#### Outliers/pontos de alavancagem\n",
    "Outliers e pontos de alavancagem s√£o pontos que se encontram longe das tend√™ncias regulares de seus dados. Estes pontos podem ter uma grande influ√™ncia na sua solu√ß√£o. Na pr√°tica, estes pontos podem at√© ser erros de digita√ß√£o. Se voc√™ estiver agregando dados de v√°rias fontes, √© poss√≠vel que alguns dos valores dos dados sejam transferidos ou agregados incorretamente.\n",
    "\n",
    "Em outros momentos os outliers s√£o pontos de dados precisos e verdadeiros, n√£o necessariamente um erro de medi√ß√£o ou de entrada de dados. Nesses casos, o ‚Äòajuste‚Äô √© mais subjetivo. Muitas vezes a estrat√©gia para trabalhar com estes pontos depende do objetivo de sua an√°lise. Modelos lineares usando o m√©todo de m√≠nimos quadrados ordin√°rios, em particular, n√£o s√£o muito robustos. Ou seja, outliers grandes podem alterar fortemente nossos resultados. Existem t√©cnicas para combater isso - amplamente conhecidas como t√©cnicas de **regulariza√ß√£o**. Elas est√£o al√©m do escopo desta aula, mas s√£o discutidas rapidamente na vers√£o gratuita do [Nanodegree de Machine Learning](https://classroom.udacity.com/courses/ud120).\n",
    "\n",
    "Um curso inteiro sobre regress√£o √© fornecido pela Penn State University e eles tomam um tempo particularmente grande para discutir o tema dos pontos de alavancagem [aqui](https://newonlinecourses.science.psu.edu/stat501/node/336/).\n",
    "\n",
    "#### Colinearidade (multicolinearidade)\n",
    "ulticolinearidade ocorre quando temos vari√°veis preditoras que est√£o correlacionadas entre si. Uma das principais preocupa√ß√µes da multicolinearidade √© que ela pode levar a coeficientes a serem invertidos da dire√ß√£o que esperamos na regress√£o linear simples.\n",
    "\n",
    "Uma das maneiras mais comuns para identificar multicolinearidade √© com gr√°ficos bivariados ou com **fatores de infla√ß√£o de vari√¢ncia (ou VIFs)**.\n",
    "\n",
    "![ibagem](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/June/5b3254fc_estatistica-regressao-linear-multipla-pt/estatistica-regressao-linear-multipla-pt.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Multicollinearity and VIFs\n",
    "\n",
    "One of the main assumptions of multiple linear regression models, is that our predictor variables are uncorrelated with one another (our x-var should be correlated with the response, but not each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df2 = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x540 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.pairplot(df[['area', 'bedrooms', 'bathrooms']]);\n",
    "# each of them has pretty strong positive relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:22:06</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:22:06   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The bedrooms has a negative coefficient associated with it.\n",
    "* Even though price and bedrooms have a positive relationship between one another, in the multiple linear regression model it showed up negative\n",
    "* The interpretation of this coefficient is now counter-intuitive to the relationship expected and what is actually true in the bivariate case.\n",
    "* This is one potential side effect of having multicollinearity in the model, is these flipped coefficients from what you expect to be true.\n",
    "* Another way to identify our predictors is correlated with one another, is **variance inflation factors** (VIFs)\n",
    "* It can be calculated for each x variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,  X = dmatrices('price ~ area + bedrooms + bathrooms', df, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['features'] = X.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Remove at least one of these last two variables as both of their variants inflation factors are larger than 10.\n",
    "\n",
    "> Vimos que quando vari√°veis ‚Äòx‚Äô est√£o relacionadas entre si, podemos inverter as rela√ß√µes em nossos modelos de regress√£o linear m√∫ltipla frente ao que esperar√≠amos quando olhamos para as rela√ß√µes bivariadas da regress√£o linear.\n",
    "\n",
    "> Para saber mais sobre VIFs e multicolinearidade, aqui est√° a publica√ß√£o referenciada do v√≠deo sobre [VIFs](https://etav.github.io/python/vif_factor_python.html).\n",
    "\n",
    "* The case that X variables were correlated with one another can lead to flipped regression coefficients from the expected relationships and inaccurate hypothesis testing results.\n",
    "* When X variables are related to one another these results can be very misleading.\n",
    "* We saw two ways to identify multicollinearity: scatterplot matrix or variance inflation factors (VIFs).\n",
    "* If you have larger than ten for a VIF then you have multicollinearity in your model. \n",
    "* VIF for a particular variable is computed as one over one minus R squared, where the R squared is computed as the R squared for that X variable being predicted by all the other X variables.\n",
    "\n",
    "$VIF_i=\\frac{1}{1-R_i^2}$\n",
    "\n",
    "$x_i=b_0+b_1x_1+b_2x_2+...+b_n+x_n$\n",
    "\n",
    " more R2 = less (1-R2) =  more $VIF_i$\n",
    "\n",
    "It's unusual to find only one large VIF in a model, because if two or more X variables are related to one anoter you would expect each of these variables to have a high VIF.\n",
    "The most common way to work with variables that are corre\n",
    "\n",
    "> 1. As rela√ß√µes esperadas entre suas vari√°veis ‚Äòx‚Äô e a de resposta podem n√£o se sustentar quando a multicolinearidade est√° presente. Ou seja, voc√™ pode esperar uma rela√ß√£o positiva entre as vari√°veis explicativas e a resposta (com base nas rela√ß√µes bivariadas), mas no caso da regress√£o linear m√∫ltipla, acontece que a rela√ß√£o √© negativa.\n",
    "> 2. Nossos resultados de teste de hip√≥tese podem n√£o ser confi√°veis. Acontece que tendo vari√°veis explicativas correlacionadas significa que nossas estimativas de coeficiente s√£o menos est√°veis. Ou seja, os desvios padr√£o (muitas vezes chamados de erros padr√£o) associados com seus coeficientes de regress√£o s√£o bastante grandes. Portanto, uma vari√°vel em particular pode ser √∫til para prever a resposta, mas por causa da rela√ß√£o que tem com outras vari√°veis ‚Äòx‚Äô, voc√™ j√° n√£o vai ver esta associa√ß√£o.\n",
    "\n",
    "## How do we know if our model fits well?\n",
    "\n",
    "It's possible to fit linear models that look like non-linear models, by adding higher order terms, like interactions $(x_1x_2)$, quadratics $(x¬≤)$, cubics $(x^3)$, and even higher order values $(x‚Å¥)$ to the model. \n",
    "It might allow us to better predict the response, and the interpretation more complex, the interpretations for lower order terms (like the slopes on square footage or on hte level of a categorical variable), are no longer easily interpreted when these terms show up in higher order terms.\n",
    "\n",
    "> **Como identificar termos de ordem superior?**\n",
    "> Termos de ordem superior em modelos lineares s√£o criados quando multiplicamos duas ou mais vari√°veis ‚Äòx‚Äô entre si. Termos comuns de ordem superior incluem quadr√°ticos $(x_1¬≤)$ e c√∫bicos $(x_1¬≥)$, onde uma vari√°vel ‚Äòx‚Äô √© multiplicada por ela mesma, assim como as intera√ß√µes $(x_1x_2)$, onde duas ou mais vari√°veis ‚Äòx‚Äô s√£o multiplicadas umas pelas outras.\n",
    "![ibagi](https://d17h27t6h515a5.cloudfront.net/topher/2017/December/5a29a5f5_screen-shot-2017-12-07-at-1.33.46-pm/screen-shot-2017-12-07-at-1.33.46-pm.png)\n",
    "> Em um modelo sem termos de ordem superior, voc√™ pode ter uma equa√ß√£o como:\n",
    "\n",
    "$\\hat{y}=b_0+b_1x_1+b_2x_2$\n",
    "> Ent√£o podemos decidir que talvez o modelo linear possa ser melhorado com termos de ordem superior. A equa√ß√£o pode mudar para:\n",
    "\n",
    "$\\hat{y}=b_0+b_1x_1+b_2x_1^2+b_3x_2+b_4x_1x_2$\n",
    "> Aqui, n√≥s introduzimos uma fun√ß√£o quadr√°tica $(b_2x_1^2)$ e um termo de intera√ß√£o $(b_4x_1x_2)$ no modelo.\n",
    "\n",
    "> Em geral, esses termos podem ajudar a ajustar rela√ß√µes mais complexas aos seus dados. No entanto, eles tamb√©m se beneficiam da facilidade de interpretar os coeficientes, como temos visto at√© agora. Voc√™ deve estar se perguntando: \"Como eu identifico se preciso de um desses termos de ordem superior?\"\n",
    "\n",
    "> Ao criar modelos **quadr√°ticos**, **c√∫bicos**, ou ordens ainda mais altas de uma vari√°vel, estamos essencialmente vendo quantas curvas existem nas rela√ß√µes entre as vari√°veis explicativas e de resposta.\n",
    "\n",
    "> Se houver uma curva, como no gr√°fico abaixo, ent√£o voc√™ vai querer adicionar uma fun√ß√£o quadr√°tica. Claramente, podemos ver que uma linha n√£o √© o melhor ajuste para esta rela√ß√£o.\n",
    "![maisibagem](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/58868097_quadraticlinearregression/quadraticlinearregression.png)\n",
    "\n",
    "> Ent√£o, se queremos adicionar uma rela√ß√£o c√∫bica, √© porque vemos duas curvas na rela√ß√£o entre a vari√°vel explicativa e a de resposta. Um exemplo disso √© mostrado no gr√°fico abaixo.\n",
    "\n",
    "> [so what?](https://tamino.wordpress.com/2011/03/31/so-what/)\n",
    "\n",
    "---\n",
    "\n",
    "## Interactions and higher order terms\n",
    "\n",
    "To fit models where the response is not linearly related to the explanatory variable we have to use **higher order terms**\n",
    "* Cubic, quadratics, interactions, where more than one variable is attached to a coeficient.\n",
    "\n",
    "To add to the model, we can simply multiply our columns by one another or squaring all the values in (quadratic relationship)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('house_prices.csv')\n",
    "df['bedrooms_squared'] = df['bedrooms']*df['bedrooms']\n",
    "df['intercept'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = sm.OLS(df['price'], df[['intercept', 'bedrooms', 'bedrooms_squared']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bedrooms' coef isn't the predicted change in price for each addtional bedroom any longer, because changing the bedroom and this term also means changing it in the quadratic term.\n",
    "* The change in price is dependent on the starting and ending number of bedrooms.\n",
    "* The change in price for changing from three to four bedrooms isn't the same as changing from five to six bedrooms. So neither of these terms is easily interpreted.\n",
    "\n",
    "We could also add a cubic term to our dataset and add to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bedrooms_cubed'] = df['bedrooms_squared']*df['bedrooms']\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'bedrooms', 'bedrooms_squared', 'bedrooms_cubed']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The r-squared increased by a little bit. But not a substantial amount for including this cube number of bedrooms;\n",
    "* Isn't appropriate to think that the cubed value of the bedrooms is associated with the price;\n",
    "* Each of the coefficients down there isn't interpreted in a nice way;\n",
    "\n",
    "**Interaction term:** Created when multiplying two or more x-variables by one another to add to the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a interaction between the area and the bedrooms number\n",
    "df['area_bed'] = df['area']*df['bedrooms']\n",
    "# Add the lower order terms to the model\n",
    "lm = sm.OLS(df['price'], df[[\n",
    "    'intercept', 'bedrooms', 'bedrooms_squared', 'bedrooms_cubed',\n",
    "    'area', 'area_bed']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a substantial improvement, how we know er should add one of these terms?\n",
    "\n",
    "---\n",
    "\n",
    "## Interpreting interactions\n",
    "* When adding higher order terms, also add the lower order terms;\n",
    "\n",
    "When adding interaction terms, you're considering it the way that a variable $X_1$ is related to your response $Y$, is dependent on another variable $X_2$\n",
    "\n",
    "The question of adding an interaction is a question about the slopes being close enough to equal. If it is, then we don't add an interaction. If it isn't, is an indication that we should add an interaction to the model.\n",
    "\n",
    "> **Termos de intera√ß√£o**\n",
    "\n",
    ">No v√≠deo anterior, voc√™ foi apresentado para a maneira como voc√™ pode interpretar as intera√ß√µes (ou como ser capaz de identific√°-las).\n",
    "\n",
    "> Matematicamente, uma intera√ß√£o √© criada pela multiplica√ß√£o de duas vari√°veis, uma pela outra, e adicionando este termo ao nosso modelo de regress√£o linear.\n",
    "\n",
    "> O exemplo do v√≠deo anterior usou **√°rea** ($x_1$) e a vizinhan√ßa (ou bairro) ($x_2$)de uma casa (sendo **A** ou **B**) para prever o **pre√ßo** ($y$) da casa. Na parte superior da tela no v√≠deo, voc√™ deve ter notado a equa√ß√£o para um modelo linear usando essas vari√°veis, como:\n",
    "\n",
    "$\\hat{y}=b_0+b_1x_1+b_2x_2$\n",
    "\n",
    "> Este exemplo n√£o envolve um termo de intera√ß√£o, e este modelo √© apropriado se a rela√ß√£o entre as vari√°veis parece como no gr√°fico abaixo.\n",
    "\n",
    "![ibagen](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/June/5b329a2d_reg.-linear-mult.-texto-interp.-pt/reg.-linear-mult.-texto-interp.-pt.png)\n",
    "\n",
    "> Onde $b_1$ √© a maneira como estimamos a rela√ß√£o entre **√°rea** e **pre√ßo**, que neste modelo que acreditamos ser a mesma, independentemente do bairro. Ent√£o $b_2$ √© a diferen√ßa de pre√ßo dependendo do local no qual est√° localizada, que √© a dist√¢ncia **vertical** entre as duas linhas:\n",
    "\n",
    "![ibagensona](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/June/5b32a41b_screen-shot-2017-12-07-at-3.56.39-pm-pt/screen-shot-2017-12-07-at-3.56.39-pm-pt.png)\n",
    "\n",
    "> Observe que, aqui, a maneira com que **√°rea** est√° relacionada com **pre√ßo** √© a mesma, independentemente da **vizinhan√ßa**.\n",
    "\n",
    ">  A diferen√ßa em pre√ßo para as diferentes **vizinhan√ßas** √© a mesma, independentemente da **√°rea**. Quando estas declara√ß√µes s√£o verdadeiras, n√£o precisamos de um termo de intera√ß√£o em nosso modelo. No entanto, precisamos de uma intera√ß√£o quando a **maneira com que a √°rea se relaciona com o pre√ßo varia dependendo do local**.\n",
    "\n",
    "> Matematicamente, quando a maneira com que a √°rea se relaciona com o pre√ßo depende do local, isto sugere que devemos acrescentar uma intera√ß√£o. Adicionando a intera√ß√£o, permitimos que as inclina√ß√µes da linha para cada vizinhan√ßa sejam diferentes, conforme mostrado no gr√°fico abaixo. Aqui, adicionamos a intera√ß√£o, e voc√™ pode ver que isto permite uma diferen√ßa entre as duas inclina√ß√µes.\n",
    "\n",
    "> Estas linhas podem at√© se cruzar ou se afastar rapidamente. Qualquer uma dessas situa√ß√µes sugeriria que uma intera√ß√£o est√° presente entre **√°rea** e **vizinhan√ßa** na maneira com que elas se relacionam com o **pre√ßo**.\n",
    "\n",
    "We will be fitting a number of different models to this dataset throughout this notebook.  For each model, there is a quiz question that will allow you to match the interpretations of the model coefficients to the corresponding values.  If there is no 'nice' interpretation, this is also an option!\n",
    "\n",
    "### Model 1\n",
    "\n",
    "`1.` For the first model, fit a model to predict `price` using `neighborhood`, `style`, and the `area` of the home.  Use the output to match the correct values to the corresponding interpretation in quiz 1 below.  Don't forget an intercept!  You will also need to build your dummy variables, and don't forget to drop one of the columns when you are fitting your linear model. It may be easiest to connect your interpretations to the values in the first quiz by creating the baselines as neighborhood C and home style **lodge**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "      <th>lodge</th>\n",
       "      <th>ranch</th>\n",
       "      <th>victorian</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price  \\\n",
       "0      1112            B  1188         3          2      ranch   598291   \n",
       "1       491            B  3512         5          3  victorian  1744259   \n",
       "2      5952            B  1134         3          2      ranch   571669   \n",
       "3      3525            A  1940         4          2      ranch   493675   \n",
       "4      5108            B  2208         6          4  victorian  1101539   \n",
       "\n",
       "   lodge  ranch  victorian  A  B  C  intercept  \n",
       "0      0      1          0  0  1  0          1  \n",
       "1      0      0          1  0  1  0          1  \n",
       "2      0      1          0  0  1  0          1  \n",
       "3      0      1          0  1  0  0          1  \n",
       "4      0      0          1  0  1  0          1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.join(pd.get_dummies(df['style']))\n",
    "df = df.join(pd.get_dummies(df['neighborhood']))\n",
    "df['intercept'] = 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.919</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.919</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.372e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 18 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:44:27</td>     <th>  Log-Likelihood:    </th> <td> -80348.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.607e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6022</td>      <th>  BIC:               </th> <td>1.607e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  348.7375</td> <td>    2.205</td> <td>  158.177</td> <td> 0.000</td> <td>  344.415</td> <td>  353.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.983e+05</td> <td> 5540.744</td> <td>  -35.791</td> <td> 0.000</td> <td>-2.09e+05</td> <td>-1.87e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>A</th>         <td> -194.2464</td> <td> 4965.459</td> <td>   -0.039</td> <td> 0.969</td> <td>-9928.324</td> <td> 9539.832</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>         <td> 5.243e+05</td> <td> 4687.484</td> <td>  111.844</td> <td> 0.000</td> <td> 5.15e+05</td> <td> 5.33e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td>-1974.7032</td> <td> 5757.527</td> <td>   -0.343</td> <td> 0.732</td> <td>-1.33e+04</td> <td> 9312.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>victorian</th> <td>-6262.7365</td> <td> 6893.293</td> <td>   -0.909</td> <td> 0.364</td> <td>-1.98e+04</td> <td> 7250.586</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>114.369</td> <th>  Durbin-Watson:     </th> <td>   2.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 139.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.271</td>  <th>  Prob(JB):          </th> <td>6.29e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.509</td>  <th>  Cond. No.          </th> <td>1.12e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.12e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.919\n",
       "Model:                            OLS   Adj. R-squared:                  0.919\n",
       "Method:                 Least Squares   F-statistic:                 1.372e+04\n",
       "Date:                Fri, 18 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        10:44:27   Log-Likelihood:                -80348.\n",
       "No. Observations:                6028   AIC:                         1.607e+05\n",
       "Df Residuals:                    6022   BIC:                         1.607e+05\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "area         348.7375      2.205    158.177      0.000     344.415     353.060\n",
       "intercept  -1.983e+05   5540.744    -35.791      0.000   -2.09e+05   -1.87e+05\n",
       "A           -194.2464   4965.459     -0.039      0.969   -9928.324    9539.832\n",
       "B           5.243e+05   4687.484    111.844      0.000    5.15e+05    5.33e+05\n",
       "ranch      -1974.7032   5757.527     -0.343      0.732   -1.33e+04    9312.111\n",
       "victorian  -6262.7365   6893.293     -0.909      0.364   -1.98e+04    7250.586\n",
       "==============================================================================\n",
       "Omnibus:                      114.369   Durbin-Watson:                   2.002\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              139.082\n",
       "Skew:                           0.271   Prob(JB):                     6.29e-31\n",
       "Kurtosis:                       3.509   Cond. No.                     1.12e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.12e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = sm.OLS(df['price'], df[['area', 'intercept', 'A', 'B', 'ranch', 'victorian']])\n",
    "results = ml.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A diferen√ßa prevista no pre√ßo de uma casa na vizinhan√ßa A em compara√ß√£o a uma casa na vizinhan√ßa C, mantendo constantes as outras vari√°veis: -194.25\n",
    "* Ao aumentar uma unidade da √°rea de uma casa, prevemos que o pre√ßo da casa aumente em 348.74 (mantendo constantes as outras vari√°veis)?\n",
    "* O pre√ßo previsto se a casa √© uma hospedaria na vizinhan√ßa C com uma √°rea de 0 (-198300).\n",
    "* A diferen√ßa de pre√ßo prevista entre uma casa vitoriana e uma hospedaria, mantendo constantes todas as vari√°veis, a hospedaria √© mais cara por 6262.73.\n",
    "\n",
    "### Model 2\n",
    "\n",
    "`2.` Now let's try a second model for predicting price.  This time, use `area` and `area squared` to predict price.  Also use the `style` of the home, but not `neighborhood` this time. You will again need to use your dummy variables, and add an intercept to the model. Use the results of your model to answer quiz questions 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['area_squared'] = df['area']*df['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['ranch' 'victorian'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1052dbe2093d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'area'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'area_squared'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ranch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'victorian'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2723\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['ranch' 'victorian'] not in index\""
     ]
    }
   ],
   "source": [
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'area_squared', 'ranch', 'victorian']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ao aumentar cada unidade na √°rea da constru√ß√£o e cada unidade de √°rea quadrada da casa, n√£o √© poss√≠vel prever se o pre√ßo aumenta ou diminui j√° que a variavel est√° associada a ordem superior;\n",
    "* Com base nos resultados, voc√™ acha que √© √∫til adicionar um termo de ordem superior para a √°rea na hora de prever o pre√ßo de uma casa? N√£o, o r-squared diminuiu.\n",
    "* A diferen√ßa prevista entre o pre√ßo da casa de fazenda e uma hospedaria, mantendo todas as outras vari√°veis constantes √© de 9917.25\n",
    "* Com o termo de ordem superior, os coeficientes associados com √°rea e √°rea ao quadrado n√£o s√£o facilmente interpret√°veis. No entanto, coeficientes que n√£o est√£o associados com os termos de ordem superior s√£o ainda interpret√°veis da maneira que voc√™ fez anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.964</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.964</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>7.983e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 17 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:08:36</td>     <th>  Log-Likelihood:    </th> <td> -81330.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.627e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.627e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th> <td>  287.5637</td> <td>    1.095</td> <td>  262.610</td> <td> 0.000</td> <td>  285.417</td> <td>  289.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>    <td> 4.535e+05</td> <td> 4261.287</td> <td>  106.427</td> <td> 0.000</td> <td> 4.45e+05</td> <td> 4.62e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1996.505</td> <th>  Durbin-Watson:     </th> <td>   1.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>8039.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.598</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 7.669</td>  <th>  Cond. No.          </th> <td>4.66e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.66e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.964\n",
       "Model:                            OLS   Adj. R-squared:                  0.964\n",
       "Method:                 Least Squares   F-statistic:                 7.983e+04\n",
       "Date:                Thu, 17 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        14:08:36   Log-Likelihood:                -81330.\n",
       "No. Observations:                6028   AIC:                         1.627e+05\n",
       "Df Residuals:                    6026   BIC:                         1.627e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "area         287.5637      1.095    262.610      0.000     285.417     289.710\n",
       "B           4.535e+05   4261.287    106.427      0.000    4.45e+05    4.62e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1996.505   Durbin-Watson:                   1.888\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8039.495\n",
       "Skew:                           1.598   Prob(JB):                         0.00\n",
       "Kurtosis:                       7.669   Cond. No.                     4.66e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.66e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml = sm.OLS(df['price'], df[['area', 'B']])\n",
    "results = ml.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* O melhor modelo deve incluir somente a √°rea, e uma vari√°vel dummy para a vizinhan√ßa B vs. as outras vizinhan√ßas.\n",
    "* A julgar pelos primeiros resultados dos dois modelos que voc√™ construiu, o melhor provavelmente envolveria apenas estas duas vari√°veis, pois seria simplificado, enquanto ainda preveria bem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

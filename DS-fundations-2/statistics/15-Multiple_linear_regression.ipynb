{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression introduction\n",
    "We can use linear algebra to predict the **$Y$** value using all multi-column at the same time. The way we do this is by creating a matrix of inputs, and we create a vector of the response that we want to predict. The matrix with the multiple column is denoted **$Y$** (capital x bold) while the vector for the response is denoted **$y$** (y bold)\n",
    "* [Linear algebra from Khan academy](https://www.khanacademy.org/math/linear-algebra)\n",
    "* [an introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/) - chapter 3\n",
    "\n",
    "In this notebook (and following quizzes), you will be creating a few simple linear regression models, as well as a multiple linear regression model, to predict home value.\n",
    "\n",
    "Let's get started by importing the necessary libraries and reading in the data you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm;\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('./house_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:08</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:08   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "mlm = sm.OLS(df['price'], df[['area', 'bedrooms', 'bathrooms', 'intercept']])\n",
    "results = mlm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Using statsmodels, fit three individual simple linear regression models to predict price.  You should have a model that uses **area**, another using **bedrooms**, and a final one using **bathrooms**.  You will also want to use an intercept in each of your three models.\n",
    "\n",
    "Use the results from each of your models to answer the first two quiz questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.269e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:09</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 9587.8878</td> <td> 7637.479</td> <td>    1.255</td> <td> 0.209</td> <td>-5384.303</td> <td> 2.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  348.4664</td> <td>    3.093</td> <td>  112.662</td> <td> 0.000</td> <td>  342.403</td> <td>  354.530</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>368.609</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 349.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.534</td>  <th>  Prob(JB):          </th> <td>1.43e-76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.499</td>  <th>  Cond. No.          </th> <td>4.93e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.93e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                 1.269e+04\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:09   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6026   BIC:                         1.691e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   9587.8878   7637.479      1.255      0.209   -5384.303    2.46e+04\n",
       "area         348.4664      3.093    112.662      0.000     342.403     354.530\n",
       "==============================================================================\n",
       "Omnibus:                      368.609   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              349.279\n",
       "Skew:                           0.534   Prob(JB):                     1.43e-76\n",
       "Kurtosis:                       2.499   Cond. No.                     4.93e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.93e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "lm1 = sm.OLS(df['price'], df[['intercept', 'area']])\n",
    "result1 = lm1.fit()\n",
    "result1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7446.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:10</td>     <th>  Log-Likelihood:    </th> <td> -85509.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.710e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.710e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-9.485e+04</td> <td> 1.08e+04</td> <td>   -8.762</td> <td> 0.000</td> <td>-1.16e+05</td> <td>-7.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td> 2.284e+05</td> <td> 2646.744</td> <td>   86.289</td> <td> 0.000</td> <td> 2.23e+05</td> <td> 2.34e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>967.118</td> <th>  Durbin-Watson:     </th> <td>   2.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1599.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.074</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.325</td>  <th>  Cond. No.          </th> <td>    10.3</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.553\n",
       "Model:                            OLS   Adj. R-squared:                  0.553\n",
       "Method:                 Least Squares   F-statistic:                     7446.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:10   Log-Likelihood:                -85509.\n",
       "No. Observations:                6028   AIC:                         1.710e+05\n",
       "Df Residuals:                    6026   BIC:                         1.710e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept  -9.485e+04   1.08e+04     -8.762      0.000   -1.16e+05   -7.36e+04\n",
       "bedrooms    2.284e+05   2646.744     86.289      0.000    2.23e+05    2.34e+05\n",
       "==============================================================================\n",
       "Omnibus:                      967.118   Durbin-Watson:                   2.014\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1599.431\n",
       "Skew:                           1.074   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.325   Cond. No.                         10.3\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2 = sm.OLS(df['price'], df[['intercept', 'bedrooms']])\n",
    "result2 = lm2.fit()\n",
    "result2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.541</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.541</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   7116.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:10</td>     <th>  Log-Likelihood:    </th> <td> -85583.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.712e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6026</td>      <th>  BIC:               </th> <td>1.712e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 4.314e+04</td> <td> 9587.189</td> <td>    4.500</td> <td> 0.000</td> <td> 2.43e+04</td> <td> 6.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 3.295e+05</td> <td> 3905.540</td> <td>   84.358</td> <td> 0.000</td> <td> 3.22e+05</td> <td> 3.37e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>915.429</td> <th>  Durbin-Watson:     </th> <td>   2.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1537.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.010</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.428</td>  <th>  Cond. No.          </th> <td>    5.84</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.541\n",
       "Model:                            OLS   Adj. R-squared:                  0.541\n",
       "Method:                 Least Squares   F-statistic:                     7116.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:10   Log-Likelihood:                -85583.\n",
       "No. Observations:                6028   AIC:                         1.712e+05\n",
       "Df Residuals:                    6026   BIC:                         1.712e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   4.314e+04   9587.189      4.500      0.000    2.43e+04    6.19e+04\n",
       "bathrooms   3.295e+05   3905.540     84.358      0.000    3.22e+05    3.37e+05\n",
       "==============================================================================\n",
       "Omnibus:                      915.429   Durbin-Watson:                   2.003\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1537.531\n",
       "Skew:                           1.010   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.428   Cond. No.                         5.84\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm3 = sm.OLS(df['price'], df[['intercept', 'bathrooms']])\n",
    "result3 = lm3.fit()\n",
    "result3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Now that you have looked at the results from the simple linear regression models, let's try a multiple linear regression model using all three of these variables  at the same time.  You will still want an intercept in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:10</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:10   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms']])\n",
    "result = lm.fit()\n",
    "result.summary() # multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Along with using the **area**, **bedrooms**, and **bathrooms** you might also want to use **style** to predict the price.  Try adding this to your multiple linear regression model.  What happens?  Use the final quiz below to provide your answer.\n",
    "\n",
    "* There is an error, because an object cannot be added to the multiple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-af8693246f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstyle_lm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'area'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bedrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bathrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresult_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstyle_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresult_style\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/regression/linear_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m                  **kwargs):\n\u001b[1;32m    816\u001b[0m         super(OLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 817\u001b[0;31m                                   hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"weights\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/regression/linear_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         super(WLS, self).__init__(endog, exog, missing=missing,\n\u001b[0;32m--> 663\u001b[0;31m                                   weights=weights, hasconst=hasconst, **kwargs)\n\u001b[0m\u001b[1;32m    664\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/regression/linear_model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \"\"\"\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegressionModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pinv_wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wendog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wexog'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLikelihoodModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hasconst'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m---> 64\u001b[0;31m                                       **kwargs)\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/model.pyc\u001b[0m in \u001b[0;36m_handle_data\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/data.pyc\u001b[0m in \u001b[0;36mhandle_data\u001b[0;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0;32m--> 633\u001b[0;31m                  **kwargs)\n\u001b[0m",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/data.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_endog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_exog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# this has side-effects, attaches k_constant and const_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/statsmodels/base/data.pyc\u001b[0m in \u001b[0;36m_convert_endog_exog\u001b[0;34m(self, endog, exog)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mexog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexog\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             raise ValueError(\"Pandas data cast to numpy dtype of object. \"\n\u001b[0m\u001b[1;32m    475\u001b[0m                              \"Check input data with np.asarray(data).\")\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPandasData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_endog_exog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "style_lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms', 'style']])\n",
    "result_style = style_lm.fit()\n",
    "result_style.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y matrix\n",
    "X = df[['intercept', 'area', 'bedrooms', 'bathrooms']]\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression closed form solution:\n",
    "\n",
    "$\\hat{\\beta}=(X`X)^-X`y$\n",
    "\n",
    "$X`$ means X transpose\n",
    "\n",
    "$()‚Åª$ means the inverstion of the results inside the parentheses\n",
    "\n",
    "In Numpy get the transpose, the inverses and the dot products to get the coeficients results for each column\n",
    "\n",
    "* [OLS in matrix form](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-293da30d4ed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(X'X)^-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(X'X)^- * X'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (X'X)^-X' * y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "beta = np.linalg.inv(np.dot(X.transpose(),X)) #(X'X)^-\n",
    "beta = np.dot(beta, X.transpose()) #(X'X)^- * X'\n",
    "beta = np.dot(beta, y) # (X'X)^-X' * y\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables\n",
    "Instead of adding columns with categorical variables, create columns with the presence or not of a variable (been the column name the varible present or not) enconded by 1 (exist) and 0 (not exist). Because the last column can be inferred from the earlier two columns, as the one values are only in the rows that didn't have ones in one of these other columns, we actually end up choosing to drop this column (which one doesn't really matter too much, noted as reference column). Without the last column, the matrix is [full rank](https://www.cds.caltech.edu/~murray/amwiki/index.php/FAQ:_What_does_it_mean_for_a_non-square_matrix_to_be_full_rank%3F)\n",
    "![image-example](https://d17h27t6h515a5.cloudfront.net/topher/2017/December/5a297de8_screen-shot-2017-12-07-at-9.43.05-am/screen-shot-2017-12-07-at-9.43.05-am.png)\n",
    "\n",
    "* The number of variables dummy added to the matrix X and the level of each categorical variable minus one have to be equal;\n",
    "* The motivation to delete one dummy variable is to garanteee that all the column are linear independent, product of $X`X$ is invertible and the matrix X be full rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1548.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:11</td>     <th>  Log-Likelihood:    </th> <td> -86683.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6025</td>      <th>  BIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.046e+06</td> <td> 7775.607</td> <td>  134.534</td> <td> 0.000</td> <td> 1.03e+06</td> <td> 1.06e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lodge</th>     <td>-7.411e+05</td> <td> 1.44e+04</td> <td>  -51.396</td> <td> 0.000</td> <td>-7.69e+05</td> <td>-7.13e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td> -4.71e+05</td> <td> 1.27e+04</td> <td>  -37.115</td> <td> 0.000</td> <td>-4.96e+05</td> <td>-4.46e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1340.120</td> <th>  Durbin-Watson:     </th> <td>   2.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3232.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.230</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.611</td>  <th>  Cond. No.          </th> <td>    3.28</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.339\n",
       "Model:                            OLS   Adj. R-squared:                  0.339\n",
       "Method:                 Least Squares   F-statistic:                     1548.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:11   Log-Likelihood:                -86683.\n",
       "No. Observations:                6028   AIC:                         1.734e+05\n",
       "Df Residuals:                    6025   BIC:                         1.734e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.046e+06   7775.607    134.534      0.000    1.03e+06    1.06e+06\n",
       "lodge      -7.411e+05   1.44e+04    -51.396      0.000   -7.69e+05   -7.13e+05\n",
       "ranch       -4.71e+05   1.27e+04    -37.115      0.000   -4.96e+05   -4.46e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1340.120   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3232.810\n",
       "Skew:                           1.230   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.611   Cond. No.                         3.28\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['A', 'B', 'C']] = pd.get_dummies(df['neighborhood']) \n",
    "df[['lodge', 'ranch', 'victorian']] = pd.get_dummies(df['style'])\n",
    "# You always drop one level from each category. This is called the baseline (the dropped column)\n",
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'lodge', 'ranch']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The intercept means that if our home is a **victorian** home, we predict its price to be 1,046,000 dollars and **lodge** is predicted to be 741,000 less than a **victorian**. A **ranch** is predicted to be 471,000 less than a **victorian**.\n",
    "\n",
    "\n",
    "`1.` Use the [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) documentation to assist you with obtaining dummy variables for the **neighborhood** column.  Then use [join](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) to add the dummy variables to your dataframe, **df**, and store the joined results in **df_new**.\n",
    "\n",
    "Fit a linear model using **all three levels** of **neighborhood** to predict the price. Don't forget an intercept.\n",
    "\n",
    "Use your results to answer quiz 1 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index([u'A', u'B', u'C'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0aa0c279160c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mneighborhood_dummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neighborhood'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneighborhood_dummies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6334\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6335\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6336\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6338\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6349\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6350\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6351\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/reshape/merge.pyc\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                          validate=validate)\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/reshape/merge.pyc\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 574\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/george/miniconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m             raise ValueError('columns overlap but no suffix specified: '\n\u001b[0;32m-> 5244\u001b[0;31m                              '{rename}'.format(rename=to_rename))\n\u001b[0m\u001b[1;32m   5245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5246\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index([u'A', u'B', u'C'], dtype='object')"
     ]
    }
   ],
   "source": [
    "neighborhood_dummies = pd.get_dummies(df['neighborhood'])\n",
    "df_new = df.join(neighborhood_dummies)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new['intercept'] = 1\n",
    "lm = sm.OLS(df_new['price'], df_new[['intercept', 'A', 'B', 'C']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.`  Now, fit an appropriate linear model for using **neighborhood** to predict the price of a home. Use **neighborhood A** as your baseline.  Use your resulting model to answer the questions in Quiz 2 and Quiz 3 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5137271516ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "lm = sm.OLS(df_new['price'], df_new[['intercept', 'C', 'B']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Observe que cada um dos coeficientes √© uma compara√ß√£o da categoria com a refer√™ncia. Portanto, um coeficiente positivo sugere que o bairro √© mais caro, em m√©dia, do que a refer√™ncia. Por outro lado, um coeficiente negativo sugere que o bairro √© menos caro, em m√©dia, do que a refer√™ncia.\n",
    "\n",
    "* Voc√™ pode olhar para os valores-p para comparar com o bairro A. Para comparar o bairro B ao bairro C, voc√™ pode comparar os intervalos de confian√ßa. Como os intervalos de confian√ßa para B e C n√£o se sobrep√µem, temos provas de que eles diferem tamb√©m.\n",
    "\n",
    "`3.` Run the two cells below to look at the home prices for the A and C neighborhoods. Add neighborhood B. This creates a glimpse into the differences that you found in the previous linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-29aa02c4c120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C == 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A == 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B == 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(df_new.query(\"C == 1\")['price'], alpha = 0.3, label = 'C');\n",
    "plt.hist(df_new.query(\"A == 1\")['price'], alpha = 0.3, label = 'A');\n",
    "plt.hist(df_new.query(\"B == 1\")['price'], alpha = 0.3, label = 'B')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now, add dummy variables for the **style** of house. Create a new linear model using these new dummies, as well as the previous **neighborhood** dummies.  Use **ranch** as the baseline for the **style**.  Additionally, add **bathrooms** and **bedrooms** to your linear model.  Don't forget an intercept.  Use the results of your linear model to answer the last two questions below. **Home prices are measured in dollars, and this dataset is not real.**\n",
    "\n",
    "To minimize scrolling, it might be useful to open another browser window to this concept to answer the quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1fc9cea67b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_new_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_new_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_new_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'intercept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lodge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'victorian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bedrooms'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bathrooms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "sty = pd.get_dummies(df_new['style'])\n",
    "new_new_df = df_new.join(sty)\n",
    "lm = sm.OLS(new_new_df['price'], new_new_df[['intercept', 'B', 'C', 'lodge', 'victorian', 'bedrooms', 'bathrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 80.9%  (r-squared) da variabilidade no pre√ßo pode ser explicada pelo modelo linear constru√≠do usando o estilo de quartos, banheiros, vizinhan√ßa e da casa;\n",
    "* Para cada quarto adicional em uma casa, espera-se um aumento de pre√ßo de 173200, em que todas as outras vari√°veis s√£o mantidas constantes;\n",
    "* Para cada banheiro adicional em uma casa, espera-se um aumento de pre√ßo de 99960, em que todas as outras vari√°veis s√£o mantidas constantes;\n",
    "* Espera-se que uma casa vitoriana custar√° 70560 a mais do que uma fazenda, sendo todo o resto igual;\n",
    "* Espera-se que uma casa na vizinhan√ßa C custar√° 7168 menos que uma casa na vizinhan√ßa A, sendo todo o resto igual.\n",
    "\n",
    "---\n",
    "\n",
    "## Dummy variables recap\n",
    "\n",
    "The biggest reason for use encoding one, zero and negative one is that it changes the coefficients that we get back from the model as well as how interpret those coefficients.\n",
    "With one-zero encoding your interpreter coefficients as a comparion to a baseline category. However, if use one-zer-negative one encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df2 = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The below function creates 1, 0, -1 coded dummy variables.\n",
    "\n",
    "def dummy_cat(df, col):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - the dataframe where col is stored\n",
    "    col - the categorical column you want to dummy (as a string)\n",
    "    OUTPUT:\n",
    "    df - the dataframe with the added columns\n",
    "         for dummy variables using 1, 0, -1 coding\n",
    "    '''\n",
    "    for idx, val_0 in enumerate(df[col].unique()):\n",
    "        if idx + 1 < df[col].nunique():            \n",
    "            df[val_0] = df[col].apply(lambda x: 1 if x == val_0 else 0)\n",
    "        else:    \n",
    "            df[val_0] = df[col].apply(lambda x: -1 if x == val_0 else 0)\n",
    "            for idx, val_1 in enumerate(df[col].unique()):\n",
    "                if idx + 1 < df[col].nunique():\n",
    "                    df[val_1] = df[val_0] + df[val_1]\n",
    "                else:\n",
    "                    del df[val_1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "      <th>ranch</th>\n",
       "      <th>victorian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7507</td>\n",
       "      <td>C</td>\n",
       "      <td>1785</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>lodge</td>\n",
       "      <td>455235</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4964</td>\n",
       "      <td>B</td>\n",
       "      <td>2996</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1489871</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7627</td>\n",
       "      <td>C</td>\n",
       "      <td>3263</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>821931</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6571</td>\n",
       "      <td>A</td>\n",
       "      <td>1159</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>299903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5220</td>\n",
       "      <td>A</td>\n",
       "      <td>1248</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>victorian</td>\n",
       "      <td>321975</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price  \\\n",
       "0      1112            B  1188         3          2      ranch   598291   \n",
       "1       491            B  3512         5          3  victorian  1744259   \n",
       "2      5952            B  1134         3          2      ranch   571669   \n",
       "3      3525            A  1940         4          2      ranch   493675   \n",
       "4      5108            B  2208         6          4  victorian  1101539   \n",
       "5      7507            C  1785         4          2      lodge   455235   \n",
       "6      4964            B  2996         5          3  victorian  1489871   \n",
       "7      7627            C  3263         5          3  victorian   821931   \n",
       "8      6571            A  1159         3          2      ranch   299903   \n",
       "9      5220            A  1248         3          2  victorian   321975   \n",
       "\n",
       "   ranch  victorian  \n",
       "0      1          0  \n",
       "1      0          1  \n",
       "2      1          0  \n",
       "3      1          0  \n",
       "4      0          1  \n",
       "5     -1         -1  \n",
       "6      0          1  \n",
       "7      0          1  \n",
       "8      1          0  \n",
       "9      0          1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = dummy_cat(df, 'style') # Use on style\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1548.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:15</td>     <th>  Log-Likelihood:    </th> <td> -86683.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6025</td>      <th>  BIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 6.421e+05</td> <td> 5854.251</td> <td>  109.677</td> <td> 0.000</td> <td> 6.31e+05</td> <td> 6.54e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td>-6.695e+04</td> <td> 8233.489</td> <td>   -8.131</td> <td> 0.000</td> <td>-8.31e+04</td> <td>-5.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>victorian</th> <td>  4.04e+05</td> <td> 7377.372</td> <td>   54.763</td> <td> 0.000</td> <td>  3.9e+05</td> <td> 4.18e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1340.120</td> <th>  Durbin-Watson:     </th> <td>   2.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3232.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.230</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.611</td>  <th>  Cond. No.          </th> <td>    1.84</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.339\n",
       "Model:                            OLS   Adj. R-squared:                  0.339\n",
       "Method:                 Least Squares   F-statistic:                     1548.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:15   Log-Likelihood:                -86683.\n",
       "No. Observations:                6028   AIC:                         1.734e+05\n",
       "Df Residuals:                    6025   BIC:                         1.734e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   6.421e+05   5854.251    109.677      0.000    6.31e+05    6.54e+05\n",
       "ranch      -6.695e+04   8233.489     -8.131      0.000   -8.31e+04   -5.08e+04\n",
       "victorian    4.04e+05   7377.372     54.763      0.000     3.9e+05    4.18e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1340.120   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3232.810\n",
       "Skew:                           1.230   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.611   Cond. No.                         1.84\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['intercept'] = 1\n",
    "\n",
    "lm = sm.OLS(new_df['price'], new_df[['intercept', 'ranch', 'victorian']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "      <th>lodge</th>\n",
       "      <th>ranch</th>\n",
       "      <th>victorian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7507</td>\n",
       "      <td>C</td>\n",
       "      <td>1785</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>lodge</td>\n",
       "      <td>455235</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4964</td>\n",
       "      <td>B</td>\n",
       "      <td>2996</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1489871</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7627</td>\n",
       "      <td>C</td>\n",
       "      <td>3263</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>821931</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6571</td>\n",
       "      <td>A</td>\n",
       "      <td>1159</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>299903</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5220</td>\n",
       "      <td>A</td>\n",
       "      <td>1248</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>victorian</td>\n",
       "      <td>321975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price  \\\n",
       "0      1112            B  1188         3          2      ranch   598291   \n",
       "1       491            B  3512         5          3  victorian  1744259   \n",
       "2      5952            B  1134         3          2      ranch   571669   \n",
       "3      3525            A  1940         4          2      ranch   493675   \n",
       "4      5108            B  2208         6          4  victorian  1101539   \n",
       "5      7507            C  1785         4          2      lodge   455235   \n",
       "6      4964            B  2996         5          3  victorian  1489871   \n",
       "7      7627            C  3263         5          3  victorian   821931   \n",
       "8      6571            A  1159         3          2      ranch   299903   \n",
       "9      5220            A  1248         3          2  victorian   321975   \n",
       "\n",
       "   lodge  ranch  victorian  \n",
       "0      0      1          0  \n",
       "1      0      0          1  \n",
       "2      0      1          0  \n",
       "3      0      1          0  \n",
       "4      0      0          1  \n",
       "5      1      0          0  \n",
       "6      0      0          1  \n",
       "7      0      0          1  \n",
       "8      0      1          0  \n",
       "9      0      0          1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_dummies = pd.get_dummies(df['style'])\n",
    "new_df2 = df2.join(style_dummies)\n",
    "new_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.339</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1548.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:03:16</td>     <th>  Log-Likelihood:    </th> <td> -86683.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6025</td>      <th>  BIC:               </th> <td>1.734e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>  3.05e+05</td> <td> 1.21e+04</td> <td>   25.120</td> <td> 0.000</td> <td> 2.81e+05</td> <td> 3.29e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ranch</th>     <td> 2.701e+05</td> <td> 1.57e+04</td> <td>   17.153</td> <td> 0.000</td> <td> 2.39e+05</td> <td> 3.01e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>victorian</th> <td> 7.411e+05</td> <td> 1.44e+04</td> <td>   51.396</td> <td> 0.000</td> <td> 7.13e+05</td> <td> 7.69e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1340.120</td> <th>  Durbin-Watson:     </th> <td>   2.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3232.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.230</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 5.611</td>  <th>  Cond. No.          </th> <td>    4.77</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.339\n",
       "Model:                            OLS   Adj. R-squared:                  0.339\n",
       "Method:                 Least Squares   F-statistic:                     1548.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:03:16   Log-Likelihood:                -86683.\n",
       "No. Observations:                6028   AIC:                         1.734e+05\n",
       "Df Residuals:                    6025   BIC:                         1.734e+05\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept    3.05e+05   1.21e+04     25.120      0.000    2.81e+05    3.29e+05\n",
       "ranch       2.701e+05   1.57e+04     17.153      0.000    2.39e+05    3.01e+05\n",
       "victorian   7.411e+05   1.44e+04     51.396      0.000    7.13e+05    7.69e+05\n",
       "==============================================================================\n",
       "Omnibus:                     1340.120   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3232.810\n",
       "Skew:                           1.230   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.611   Cond. No.                         4.77\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df2['intercept'] = 1\n",
    "\n",
    "lm2 = sm.OLS(new_df2['price'], new_df2[['intercept', 'ranch', 'victorian']])\n",
    "results2 = lm2.fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns pontos a serem observados: Em primeiro lugar, a programa√ß√£o 1, 0 significa uma compara√ß√£o com a categoria de base. Depois, a programa√ß√£o 1, 0,-1 significa uma compara√ß√£o com a m√©dia geral. Por fim, a linguagem de aumento de uma unidade est√° associada √†s vari√°veis quantitativas, e n√£o √†s vari√°veis categ√≥ricas.\n",
    "\n",
    "* 33.9% da variabilidade no pre√ßo pode ser explicada pelo estilo de casa.\n",
    "* 642100 √© o pre√ßo m√©dio de moradia previsto, sem levar em conta o estilo.\n",
    "* Em compara√ß√£o a uma hospedaria, prevemos que uma casa vitoriana tenha uma alta de pre√ßo de 741100, mantendo todo o resto constante.\n",
    "* Em compara√ß√£o a uma casa mediana, prevemos que o pre√ßo de uma casa vitoriana seja 404000 maior, mantendo todas as outras vari√°veis constantes.\n",
    "\n",
    "Para prever a categoria de refer√™ncia na codifica√ß√£o 1, 0, voc√™ tem que utilizar o intercepto. Na codifica√ß√£o 1, 0, -1, voc√™ precisa multiplicar cada coeficiente categ√≥rico por -1 para chegar na categoria que falta. Com isto em mente, qual √© o pre√ßo m√©dio previsto para hospedarias utilizando o modelo de codifica√ß√£o 1, 0, -1?\n",
    "\n",
    "* Multiplicando -1 pela fazenda e pela casa vitoriana, obtemos o seguinte resultado: 642100 + 66950 - 404000 = 305050. Observe tamb√©m que isso coincide com a mesma previs√£o (erro de 50 devido aos arredondamentos), que voc√™ v√™ no modelo de codifica√ß√£o 0,1\n",
    "\n",
    "---\n",
    "\n",
    "## Potential problems introduction\n",
    "\n",
    "There's a number of problems that may arise. First, what is your model for?\n",
    "* To understand if your X and Y variables are related?\n",
    "* To best predict the response variable?\n",
    "* Find which variables are really useful in predicting your response?\n",
    "\n",
    "Depending on which aspects you're most interesed in, this can help determine which problems you actually care about addressing.\n",
    "\n",
    "* A linear relationship may not exist between your response and predictor variables;\n",
    "* You might have correlated errors;\n",
    "* You might not have constant variance of your errors;\n",
    "* You might have outliers or leverage points that hurt your model;\n",
    "* You might have multicolliearity.\n",
    "\n",
    "Chapter 3 of \"[An introduction to statistical learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf)\" dives into each of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. N√£o-linearidade das rela√ß√µes entre preditor e resposta\n",
    "2. Correla√ß√£o dos termos de erro\n",
    "3. Vari√¢ncia n√£o-constante e erros normalmente distribu√≠dos\n",
    "4. Outliers/pontos de alta alavancagem\n",
    "5. Colinearidade\n",
    "\n",
    "### Linearidade\n",
    "A suposi√ß√£o de linearidade √© que o modelo linear representa a verdadeira rela√ß√£o existente entre a vari√°vel de resposta e a preditora. Se isso n√£o for verdade, ent√£o as suas previs√µes n√£o ser√£o muito precisas. Al√©m disso, as rela√ß√µes lineares associadas aos seus coeficientes tamb√©m n√£o s√£o muito √∫teis.\n",
    "\n",
    "Para avaliar se uma rela√ß√£o linear √© razo√°vel, um gr√°fico dos res√≠duos $(y-\\hat{y})$ pelos valores preditos (\\hat{y}) geralmente √© √∫til. Se existem padr√µes de curvatura neste gr√°fico, isso sugere que um modelo linear pode n√£o se ajustar adequadamente aos dados, e alguma outra rela√ß√£o existe entre as vari√°veis preditoras e de resposta. Existem muitas maneiras de criar modelos n√£o-lineares (at√© mesmo usando o formato do modelo linear), e voc√™ ser√° apresentado a algumas delas.\n",
    "\n",
    "Na imagem na parte inferior desta p√°gina, os modelos s√£o considerados viesados. O ideal seria que tiv√©ssemos uma dispers√£o aleat√≥ria de pontos como na figura do gr√°fico de res√≠duos do canto superior esquerdo.\n",
    "\n",
    "#### Erros correlacionados\n",
    "Os erros correlacionados frequentemente ocorrem quando nossos dados s√£o coletados ao longo do tempo (como na proje√ß√£o de pre√ßos de a√ß√µes ou taxas de juros futuras) ou quando os dados s√£o relacionados espacialmente (como a previs√£o de regi√µes de inunda√ß√µes ou secas). Muitas vezes podemos melhorar nossas previs√µes usando informa√ß√µes dos √∫ltimos pontos de dados (para o tempo) ou os pontos nas proximidades (para o espa√ßo).\n",
    "\n",
    "O principal problema em n√£o levar em conta os erros correlacionados √© que voc√™ poderia utilizar essa correla√ß√£o para sua vantagem, prevendo de uma maneira melhor os eventos futuros ou eventos espacialmente pr√≥ximos uns dos outros.\n",
    "\n",
    "Um dos jeitos mais comuns de identificar se os erros s√£o correlacionados √© verificar o dom√≠nio de onde os dados s√£o coletados. Se voc√™ n√£o tiver certeza, h√° um teste conhecido como teste [Durbin-Watson](https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) que √© comumente usado para avaliar se a correla√ß√£o dos erros √© um problema. Depois disso, os modelos [ARIMA ou ARMA](http://www.statsref.com/HTML/index.html?arima.html) s√£o comumente implementados para usar esta correla√ß√£o em previs√µes melhores.\n",
    "\n",
    "#### Vari√¢ncia n√£o-constante e erros normalmente distribu√≠dos\n",
    "Vari√¢ncia n√£o-constante ocorre quando a propaga√ß√£o dos valores previstos difere dependendo de qual √© o valor que se est√° tentando prever. Isto n√£o √© um problema enorme em termos de uma boa previs√£o. No entanto, isso leva a intervalos de confian√ßa e p-valores que s√£o imprecisos. Os intervalos de confian√ßa para os coeficientes ser√£o amplos demais para √°reas onde os valores reais est√£o mais perto dos valores previstos, mas ser√£o muito estreitos para √°reas onde os valores reais est√£o mais separados dos valores previstos.\n",
    "\n",
    "Comumente, um logaritmo (ou alguma outra transforma√ß√£o da vari√°vel de resposta) √© feita para ‚Äúse livrar‚Äù da vari√¢ncia n√£o-constante. A fim de escolher a transforma√ß√£o, um [Box-Cox](http://www.statisticshowto.com/box-cox-transformation/) √© geralmente usado.\n",
    "\n",
    "A vari√¢ncia n√£o constante pode ser avaliada novamente, usando um gr√°fico dos res√≠duos pelos valores previstos. Na imagem na parte inferior da p√°gina, a vari√¢ncia n√£o-constante √© rotulada como **heterosced√°stica**. Idealmente, queremos um modelo n√£o viesado e com res√≠duos homoced√°sticos (consistente em toda o intervalo de valores).\n",
    "\n",
    "Embora o texto n√£o aborde a normalidade dos res√≠duos, esta √© uma suposi√ß√£o importante da regress√£o se voc√™ est√° interessado em criar intervalos de confian√ßa adequados. Mais sobre este tema √© fornecido [aqui](http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm).\n",
    "\n",
    "#### Outliers/pontos de alavancagem\n",
    "Outliers e pontos de alavancagem s√£o pontos que se encontram longe das tend√™ncias regulares de seus dados. Estes pontos podem ter uma grande influ√™ncia na sua solu√ß√£o. Na pr√°tica, estes pontos podem at√© ser erros de digita√ß√£o. Se voc√™ estiver agregando dados de v√°rias fontes, √© poss√≠vel que alguns dos valores dos dados sejam transferidos ou agregados incorretamente.\n",
    "\n",
    "Em outros momentos os outliers s√£o pontos de dados precisos e verdadeiros, n√£o necessariamente um erro de medi√ß√£o ou de entrada de dados. Nesses casos, o ‚Äòajuste‚Äô √© mais subjetivo. Muitas vezes a estrat√©gia para trabalhar com estes pontos depende do objetivo de sua an√°lise. Modelos lineares usando o m√©todo de m√≠nimos quadrados ordin√°rios, em particular, n√£o s√£o muito robustos. Ou seja, outliers grandes podem alterar fortemente nossos resultados. Existem t√©cnicas para combater isso - amplamente conhecidas como t√©cnicas de **regulariza√ß√£o**. Elas est√£o al√©m do escopo desta aula, mas s√£o discutidas rapidamente na vers√£o gratuita do [Nanodegree de Machine Learning](https://classroom.udacity.com/courses/ud120).\n",
    "\n",
    "Um curso inteiro sobre regress√£o √© fornecido pela Penn State University e eles tomam um tempo particularmente grande para discutir o tema dos pontos de alavancagem [aqui](https://newonlinecourses.science.psu.edu/stat501/node/336/).\n",
    "\n",
    "#### Colinearidade (multicolinearidade)\n",
    "ulticolinearidade ocorre quando temos vari√°veis preditoras que est√£o correlacionadas entre si. Uma das principais preocupa√ß√µes da multicolinearidade √© que ela pode levar a coeficientes a serem invertidos da dire√ß√£o que esperamos na regress√£o linear simples.\n",
    "\n",
    "Uma das maneiras mais comuns para identificar multicolinearidade √© com gr√°ficos bivariados ou com **fatores de infla√ß√£o de vari√¢ncia (ou VIFs)**.\n",
    "\n",
    "![ibagem](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/June/5b3254fc_estatistica-regressao-linear-multipla-pt/estatistica-regressao-linear-multipla-pt.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Multicollinearity and VIFs\n",
    "\n",
    "One of the main assumptions of multiple linear regression models, is that our predictor variables are uncorrelated with one another (our x-var should be correlated with the response, but not each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./house_prices.csv')\n",
    "df2 = df.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAIUCAYAAABGj2XYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3Xt4HPV5L/DvOzO7q9XFtizLTrBMbBLj1KQyWEoKpiclkHJoIeXk2AEnNiYk9TWBlORw6YVDmksfwE0JgdjGTsLNTsDYSUMhJXAgTtpQEiQDburEmLsFKRayDLYsaXdm3vPH7g5aa1cz2t3R7krfz/Po8e7sXH4rXkbv/K6iqiAiIiIqBaPcBSAiIqLxg4kFERERlQwTCyIiIioZJhZERERUMkwsiIiIqGSYWBAREVHJhJpYiMiVIvJfIvIbEfmBiNSIyBwR+ZWI7BeR+0Qkmt43ln7/fPrz2UPO89fp7ftE5H+GWWYiIiIqXGiJhYjMBHAFgHZV/QAAE8BSADcCuFlV5wLoBfDZ9CGfBdCrqu8DcHN6P4jI/PRxpwA4D8AGETHDKjcREREVLuymEAtAXEQsALUAfg/gbAA70p/fBeB/pV9fmH6P9OfniIikt9+rqoOq+hKA5wF8KORyExERUQFCSyxU9TUA/wjgVaQSircAdAI4rKp2ercuADPTr2cCOJA+1k7v3zR0e45jPCKySkQ6RKTjlFNOUQD84U+un7JgfPIn4E9ZMD75E/AnkDCbQhqRqm2YA+AEAHUA/izHrpnCSp7P8m3P3qC6WVXbVbU9Ho8XVmiikDA+qZIxPqmUwmwK+SiAl1S1W1WTAH4IYBGAKemmEQBoAfB6+nUXgFkAkP58MoBDQ7fnOIaIiIgqSJiJxasATheR2nRfiXMA7AXwMwBL0vtcCuDH6dcPpN8j/fnjmloh7QEAS9OjRuYAmAvg1yGWm4iIiApk+e9SGFX9lYjsALAbgA3gaQCbATwE4F4R+Vp623fTh3wXwD0i8jxSNRVL0+f5LxHZjlRSYgP4nKo6YZWbiIiIChdaYgEAqno9gOuP2/wicozqUNUBAJ/Ic56vA/h6yQtIREREJcWZN4mIiKhkQq2xGE9mX/vQqPZ/+YbzQyoJEVUT11X09CWQsB1ELRNNdVEYRq7BbkRjL4z4ZGJBRBQS11Xse+MIVt7dga7efrQ0xrFlRTvmzWhgckFlF1Z8MrEgIgpJT18CNz+6D9ddMB9T4hEc7k/i5kf34esfb0VzQ6zcxaMJLqz4ZGJBRBQS13Vx6aI5uGbnHu+J8MbFrXBdt9xFIwotPtl5k4goJI7Cu2kDQFdvP67ZuQdO4MmRicITVnyyxoKIKCSqiub6WFZV86ZdLyA19x9ReYUVn0wsiIhCEo+auPq8ebhqxztVzeuXtCIeNctdNKLQ4pNNIUREIbFd9W7aQKqq+aode2C7rLGg8gsrPplYEBGFJGm73k07o6u3H0mbnTep/MKKTyYWREQhiVgGWhqzlyFvaYwjYvHWS+UXVnwyuomIQmIawPolrd7NO9OGbfLOSxUgrPhk500iopD0DTq46eHsCYhuengfbvnkaWiqK3fpaKILKz6ZWBARhcQUQffRQay+p9Pb1tIYh8nZvKkChBWfrJAjIgpJPGri5osWZFU133zRAg43pYoQVnyyxoKIKCSTYhE01kXx1Qs/gNqoiWMJB411UUyKRcpdNKLQ4pOJBRGRj0KXlj7Un8Cn73gqa0hfS2McP1y3CNMbasIsMo0TYSxrnhFWfDKxICIaQTFLSw8knZzzBAwkOY8F+QtrWfOMsOKTfSyIiEbQ05fwbuxA6sa78u4O9PQlfI81RXLOE8DOmxREMbEXRFjxycSCiGgECTv3U13CdnyPjZhGznkCIpzIggIoJvaCCCs+2RRCRDSCqGXi3PnTsbhtljfWf2fnAUQt/57zqoraqJnVOa42anJ1UwqkmNgLIqz4ZGJBRDSCxngEV5xzMtZs7fTauTctb0Nj3L/n/IDtYtuTr2Llh0+CaQgcV7HlFy/i82e/bwxKTtWumNgLIqz4ZGJBRDSC3v4k/uWZLtzx6Q96N98dHa9ixoffh+aG2IjHxiwDH184E5fd+VTWstRRrhVCARQTe0GEFZ9MLIiIRiBQnL8g++a7YdlCCPyri908y1JvX3V62MWmcaCY2AsirPhkYkFENIJB28W6bbuzbr7rtu3GfQFuvraraK6PZa3FsGnXC7Bd9rEgf8XEXhBhxScTCyKiEdiu5uyZH+TmWxMxcPV587ynwkxVc02ETSHkr5jYCyKs+Aw1ukVkiojsEJHfichvReQMEZkqIo+KyP70v43pfUVEviUiz4vIHhFZOOQ8l6b33y8il4ZZZiKioSwj91h/K8AERY6LnFXNDufHogCKib0gworPsGssbgHwsKouEZEogFoAfwPgMVW9QUSuBXAtgGsA/BmAuemfPwKwEcAfichUANcDaAegADpF5AFV7Q257EREqIuZ2LhsIdamq6RbGuPYuGwh6mL+Q/4SjotFJzUN63WfYGZBARQTe0GEFZ+hJRYiMgnAhwF8GgBUNQEgISIXAjgrvdtdAHYhlVhcCOBuTQ2gfTJd2/Hu9L6Pquqh9HkfBXAegB+EVXYiGp8KWXehLmJhcm0Ed172IRgCuApELUFdxP/2WRs1sfyM9wzrfFfL1U0pgGJiL4iw4jPMppCTAHQDuENEnhaR74hIHYAZqvp7AEj/Oz29/0wAB4Yc35Xelm97FhFZJSIdItLR3d1d+m9DVATGZ/m5ruLlnj785rW30NXbj9+89hZe7umD69Ne/WZfAtv+42Vv0iBVxbb/eBlvBphWOZGn813CrqwaC8ZnZSom9oIIKz7DbAqxACwEcLmq/kpEbkGq2SOfXI8NOsL27A2qmwFsBoD29nZ2uaaKwvgsv8P9Cbzx9gCu+/FvsjqqTamNYGpd/jkBTAM5h/wFmfU47M53pcL4rEzFxF4QYcVnmDUWXQC6VPVX6fc7kEo03kg3cSD978Eh+88acnwLgNdH2E5EFFh/wsnZUa0/MfK6C0lHcz7VJR3/m2/Yne9ofCsm9oIIKz5DSyxU9b8BHBCReelN5wDYC+ABAJmRHZcC+HH69QMAVqRHh5wO4K10U8lPAZwrIo3pESTnprcREQXmaO6nM797dDFPdRFLsGHZwqxFnjYsW4iIxcSC/IVd4xVWfIY9KuRyANvSI0JeBHAZUsnMdhH5LIBXAXwive9PAPw5gOcBHEvvC1U9JCJfBfBUer+vZDpyEhEFVRMx0dIYz7pRtzTGfcfsR0wj53FBVoC0HeChZ18bNiXzpWeeVPgXoQkjU6NwfOyVqsYrrPgMNbFQ1WeQGiZ6vHNy7KsAPpfnPN8D8L3Slo6IJpJpdTFsWdGOlXd3eO3VW1a0Y9oI/SsAIGZJziF/sQBPdYYAH543I6uN/MbFrWBLCAVREzFyxl6pJlgLKz458yYRTQiGIZjbXI/tq89A0nERMQ1Mr4/5DjcdtF3U15j4wcrT4ajCFIHtOoF6zjuu4hf73hj2RDi7aU6pvhaNY4O2i8a6CO5ddTocV2EaAkBLNqoorPhkYkFEE4LrKvYfPIqV9wypsbikHfPe1TBicmGK4M0jCVy5/VnvuJsvWoATp9b6XrMmYuBjp7ZkPRFuWt7GKb0pkKhp4PdvDQyrsXj35JqSnD+s+GR0E9GE8ObRQS+pAFKd4Fbe04E3jw6OeJzjqpdUZI67cvuzcAJ0oBtMuliztTPr2DVbOzGYrKx5LKgyDdqul1QAqfhZu203BktUYxFWfDKxIKIJoT/p5OxhP5AcebipnWc0ia3+iUWySuaxoMoU9qiQsOKTTSFENCGYhuDc+dOxuG2Wt0T0zs4Dvn0sLMnTM1/8e7jl69VvsvcmBWDlidlSjQoJKz5ZY0FEE0JDzMDl55yMrz64FxdvfhJffXAvLj/nZDTERr4NGoZg/ZLWrLH+65e0+iYkQOrGnetYTpBFQcSjuWM2Hi3Nn+6w4pM1FkQ0IRwddLH2uPbktVs7cd+q0zF5hH6Yg7aLmx7eh+sumO89Nd708D58c+mpvtfMd+wtAY4lOpbIHbP3rjodU+uKP39Y8cnEgogmhHzt1X6dMCOGoPvoIFbf0+lta2mMIxKkxsLMfaxlssaC/Dl5YtZv4bygwopPNoUQ0YQQKXBdhJiVmqRoaHVxaoIs/9unIYJbP3la1rG3fvI0GAH6ZxBF07O+DhV01tcgwopP1lgQ0YQQtQxsWLbQW9Qpsy5C1CdBSLqac5KiZJC1QgxBXczEVy/8AGqjJo4lHNTFzEC1HURRS3DHp9vR1TvgxU9LYw2iJVprJqz4ZGJBRBPCgO3itsf3Z7Un3/b4fvzfj50y4nGGAL19yWGTFM2YNPJU4AAw6Cg+c2fHsF7321efUfT3ofHPVWAg6eK6H/8mK/ZKNVo5rPhkYkFEE4LjKh7ZexCP7D2Ytf3vzp8/4nFJR3NOUnTfqtN9r5l0XDTXx7KSmU27XoDtcIIs8pfIM0FWkNgLIqz4ZGJBRBNCoWP2i5mkKB4xcfV583DVjj3eE+f6Ja2oiZiFfQmaUMKeICus+GRiQUQTgmUIbl++EAePJLz25OkNUd/Om8UsXe246t20gdQfhat27MH9bAqhAMJeNj2s+OSoECKaEEQwrKNm1DLg1wE+lu70ObTn/IaAo0Js183zxMmmEPJXGzWwaXlbVuxtWt6G2hJNkBVWfLLGIiSzr30o8L4v33B+iCUhIgBQBd48msjqCLd+SSsm1URGPM5xFTFLcOdlH4IhqQ51jusEWoTMyDMdOIebUhAJW+G4btaoDcd1kbBL0xQSVnyyxoKIJoSkq7jjly/hugvm475Vp+O6C+bjjl++5DtsdNB2ce3O3+CF7qPoPjKIF7qP4tqdvwm0wqQIcOPi7CmTb1zc6ltLQgSkYvbbP3seiXRnyoTj4ts/ez7QUOcgwopP1lgQUVVJJh0cPDoI21VYhmB6fQyRAJ3NDAEuXTQH1+x8p6PajYtb4TfJYMQ0cs+8GWCSIlXgF/vewB2f/iBMQ+C4ih0dr2J20xzfY6l6FBqTfgqN2aDCik8mFkRUNZJJB787eNRbP6GlMY6Ny9vw/un1/jdyBe564qWsoXV3PfESvhxgHov1S1qH9ZwP0n9uWm0UF5zagsvufCqrvNNqo6P41lTJiopJPwXGbFBhxSebQoioahw8OohbH3suqznj1seew8Gjg77HigDrPvI+RNM1DVHTwLqPvM+32ndgyEJNmWve9PA+DARoCnkrYedcROqthO3/ZakqFBOTfgqN2aDCik/WWBBR1TAE+OK5J8MyTBgCNNXH8MVzTw5Ue2CIoD/hDOu86ddRzTIEzQ3ZT3DNAYapAsBg0snZ634wyVEh40UxMel/7sJiNqiw4pOJBRFVjYhpIGEr/nLbr7PW+wjS3yGZZ8z+vT6zGMajBi4/5+RhVd3xAEP+8ve69z2UqkQxMemn0JgNKqz4ZFMIEVWNQdv1FhEDUjfaddt2BxqhkW8Jar9ho8cSbs7q4mMJ/2saRu5e9wYzi3GjmJj0U2jMBhVWfLLGgoiqRjFTHEfSS1Af/3Tm92Tp5rmmG+CapmHk7Hz3tY//oe+xVB3CnHa70JgNKqz4ZGJBRFXDMgTnzp+OxW2zvBvhzs4Dgfo7RE3BxmULh61SGvUZu2cWuMYIAEyri+HKP52HlXd3eNfcsqId0+r8V0al6lBMTPopNGaDCis+AyUWIlID4LMATgFQk9muqp8JcKwJoAPAa6p6gYjMAXAvgKkAdgO4RFUTIhIDcDeANgA9AC5W1ZfT5/jr9PUdAFeo6k8Df0MiGjcaagxccc7JWDOkv8Om5W1oqPF/ghuwXdx63LLptwZZNt0AvnNpm9c5z1XAdh0YAR4aDUMwb0YDfrTuTCRsB1HLRFNdlE0h40gxMelnwHbx4LOvDZtn4pJFpZkHJaz4DFpjcQ+A3wH4nwC+AmAZgN8GPPYL6X0npd/fCOBmVb1XRDYhlTBsTP/bq6rvE5Gl6f0uFpH5AJYildScAOD/icjJquoEvD4RjRN9g+rdwIFUlfOarZ24f/UZmBQf+dhCl02PGgYcF/jLu97pnHf7JW2IBskskLp5NzewhmK8KiYm/RgCfHjejKx5Jko5QRYQTnwGTSzep6qfEJELVfUuEfk+AN9aAxFpAXA+gK8D+KKICICzAXwqvctdAL6MVGJxYfo1AOwAcFt6/wsB3KuqgwBeEpHnAXwIwH8ELDsRjRNJJ/eiSUnHv6Nc1DRyVln7tVcnXcXqe7L/cKy+pxM71gRbAdJ1FT19CdZYjFPFxKQfzTNB1vUlmiALCCc+gyYWyfS/h0XkAwD+G8DsAMd9E8DVABrS75sAHFbVzOwbXQBmpl/PBHAAAFTVFpG30vvPBPDkkHMOPcYjIqsArAKAE088MdCXIhorjM/SsMw8y0gHeISLmILPnz3X68H/zrDAkY8dtHP/4UgE6PXvuop9bxwZ1oY9b0ZDRSUXjM/CFROTfvJN6V2q0AkrPoMmFptFpBHAdQAeAFAP4P+OdICIXADgoKp2ishZmc05dlWfz0Y65p0NqpsBbAaA9vb20ozFISoRxmdpWIbg2586DYf6kt5qj1PrIoE6yg3kGRboNydAvs6bQW68PX0J76aduebKuzvwo3VnVlTzCOOzcMXEpB9X4SUVQCp+rtm5B/eVaB6LsOIzUGKhqt9Jv/w5gJMCnvtMAH8hIn+OVIfPSUjVYEwREStda9EC4PX0/l0AZgHoEhELwGQAh4Zszxh6DBFNILarSDqaNRPhNy8+NdC4flfzDBvVkY+tsQxsWLZwWE1HjeXfxyJh557ZMGGzi9h4UUxM+ik0ZoMKKz6DjgqZAeAfAJygqn+W7lB5hqp+N98xqvrXAP46ffxZAP6Pqi4TkfsBLEFqZMilAH6cPuSB9Pv/SH/+uKqqiDwA4Psi8k9Idd6cC+DXo/6mRFT9FPj94T7cu+p0OK7CNARPv9KDEybX+B5q5pll0PSZHtl2FbcdN5rktsf34ysXfsD3mlHLzHnNqFX8ypdUIYqIST/5Z8YsTVtIWPEZdDzMnUh11jwh/f45AH9V4DWvQaoj5/NI9aHIJCffBdCU3v5FANcCgKr+F4DtAPYCeBjA5zgihGhiikcNzG6ehKWbn8SfrN+FpZufxOzmSYGm146k5wQYOsvgxoB9LB7ZexCr7+nExZufxOp7OvHI3oOBZlZsqotiy4r2rGtuWdGOpjqubjpeFBOTfgqN2aDCis+gfSymqer29HwSmc6Vgf+4q+ouALvSr19EalTH8fsMAPhEnuO/jtTIEiKawI4lXDz4TNewcf0rFs1BY93Ix6qmOsN99cIPeG3hhqS2j6SYCbIAIGYZWdeMBWhCoepRTEz6KTRmRyOM+AyaWPSJSBPSnSZF5HQAbxV9dSKiUYiYgvMXzMwa1x9kZAeQatJYvXX3sATBryNcxBCsX9LqLQaVWWEyErDz5orv/XrYNSut8yYVrpiY9FNozAYVVnwGTSy+iFQfiPeKyC8BNCPVD4LKYPa1D41q/5dvOD+kkhCNraSjOUd2BLnR5lvTwa+TnYhgUo2V9VQ3qcaCBGjnZufN8a+YmPRTaMwGVbbOmyJiIDWq408AzENq+Oc+VU2OeCARUYk5rqK5PpbVkXLTrhcC3WgLHTaqqohaBmZNrfWm9FZ1oQHqo9l5c/wrJib9FDPUOYiw4tM3sVBVV0S+oapnAPivoq5GRFSEmGXg6vPmDWuWCNIuHM3TpBH1uUlHLAOGIVnt2oYhiAS4ZlNdFHd/5kN4peeYV9vxnqZadt4cR4qJST+FxmxQYcVn0KaQR0RkMYAfapA0nYgoBLar3k0WSFXbXrUj4IRBAtRGzawmjdqomXsKvqzDFH2DzrB5LKbWBrsVDtpu1hwHW1a0BzqOqkNRMemnwJgdjTDiM2hK9UUA9wMYFJG3ReSIiLxd9NWJiEahmDbn/qSLLz+wF4n0Gg4JJ/V+IDnysNFjidwzdh5L+A83zTezYU9fwvdYqg5h9oMoNGaDCis+g8682SAiU5GanKr4WT+IiAoQydPmHGT6ZNMQdB8dxOp7OrOO9WuvzveHww7wh4OdN8e/YmLST6ExG1RY8RmoxkJE/hKp6bwfRmoF0ofhs1YIEVGpiQDfWnpa1oQ+31p6GoJMRJgZNjr02CDDRq30H46hgv7hiFhGzmOD9M+g6lBMTPopNGYDnz+k+Azax+ILAD4I4ElV/YiIvB/A3xd1ZSKa0ApZrjlqGqivyW5zrq8xEfVZ+hwAHFXEj2uvjkdN33UXopaBjcsWYu2QPhYbly1ENMDN18rT+a4UT7M0OmEtX19MTPopNGaDCis+g37zgfTMmBCRmKr+Dqmhp0REo5ZZrvnjG36JM2/8GT6+4ZfY98YRuD7NCwO2i3/86b6sNud//Ok+DASYXhsK/Oue19HSGEdzQwwtjXH8657XfWcxdF1FfY2JH6w8HT+/6iz8YOXpqK8xfcsKAP0JBzc9vA/XXTAf9606HdddMB83PbwP/Qk2hYylQuMtiKJi0k+BMRtUWPEZtMaiS0SmAPhnAI+KSC+4wigRFejNvsGcncZ+uG4Rpjfk78YlAly6aI63lHRLYxw3Lm4N1hRiGblnSPSpebBMgTMIvPzmUe+pcdbUOKwAMytGLTNnGznnsRhbhcZbEMXEpJ9CYzaosOIzUOlU9eOqelhVvwzgOqQWDPtfRV2ZiCasgWTuTmMDyZGflFTh3cAzx1yzc0+gJ7hB2/VWKc08nd32+H7fxcRsR9F9ZBDX/fg3uHjzk7jux79B95FB2I7/RbkIWWUoNN6CKCYm/RQas0GVexEyj6r+vKgrEtGEV+hy0E6+oX0B7uJGnidLv4qHZJ55Cu4NME+BYQjmzWjAj9adWfK2fQouzOXHi4lJP4XGbODzhxSf7JpMRGMubhnYcNxy0BuWLUTcp4rXzDNCwwzwByLfk6VfM3u+PxxB2+cNQ9DcEMPMxlo0N8SYVJRBofEWRDEx6afQmB2NMOKTiQURjbmEozmreBM+zQs1UQMbl7dl/YHYuLwNNdFgo0JyJgg+T5axPEPygowKocpQaLwFUUxM+ik0Zstt1E0hRETFsl0XH33/dJxywiQ4rmJmYxxv9U2H7Y7cdjyQcPFy99u4d9XpcFyFaQiefqUHU2ubgLqRr2nlqQ73e7IUALd+8jRc/oOnveroWz95WilnVaaQFRpvQRQTk34KjdlyY2JBRGNuUo2JP5g5BUs3P/nO3BDL2zCpZuTe6BFTMHtaQ/ZxyxYiEqDR2TAE3/7UaTjUl/RGd0yti/hW/Q46Lr7yL3uzVq/8yr/sxS2fPG1U35nKp9B4C6KYmPRTaMyWGxMLIhpzRwddPPhMF+749AdhGgLHVezoeBUrFs3B5Nr8x9kucGu6SjvzR/7Wx/fjy3/xAd9ruqoYSGYvuPSNTyzwXf68JpJ7SF5NhE0h1aLQeAuimJj0U2jMlhsTCyIacxFTcMFx4/ODPeUpPvvHJ+FL9z+bdaMV+N9oVeEdB6Taqr90/7O+q1BOq4thy4p2bx6EzJC8aXWxoF+XyqzweAui8Jj0PXOBMVtuTLmJaMwlHfWmyAZSN8y123Yj6deZLs+NNsgDXL6OcH7DAocOyfvlNR/Bj9adiXkzGiq+OpreUXC8BVFETPopNGbLjYkFEY25QpeaThax0qgphQ8L5JDR6hbm0ubFxKSfYmK2nNgUQkRjzjIEq//HbCxpPzGrzdv0+YOdmTNg2ERHAf7QGwLcuLh12GRDBh+vxr1C4y2IYmLST7XGLBMLIhpzNVEDF5zakt3mHWDsf9TMvRpjNEBbuavAXU+8lNXJ7q4nXsL1HzulVF+LKlSh8RZEMTHpp1pjlokFEY25gYSLtVs7s9u8t3amOqWNMPZfFZhcG8laRnpybSRQe7ZlCC47c05BS0SHteQ2jY1C4y2IYmLSTzExW05MLIhozOVr8/ZrlxYR6HH7qKuQAG3Og7brLRGdefq76eF9uGXpqSMel1ly+/hRIezAWT0KjbcgiolJP4XGbLmFlliIyCwAdwN4FwAXwGZVvUVEpgK4D8BsAC8DuEhVeyX1X+EWAH8O4BiAT6vq7vS5LgXwd+lTf01V7wqr3EQUPitPu7Tfk1hdFHg7kj2pUSxiIshijJYhOeej8LtmT18i55LbP1p3JpobOOS0GhQab0EUE5N+Co3ZcguzxsIG8CVV3S0iDQA6ReRRAJ8G8Jiq3iAi1wK4FsA1AP4MwNz0zx8B2Ajgj9KJyPUA2gFo+jwPqGpviGUfU7OvfajcRSAqSKFNBBFTsHF5m1c9nWnzjlgjH9uXBH77+mGc9p6mrOmTJ8WbUVcz8jUtU7BpeRvWDLnmpuVtsHzawhN27iW3E3bxS27T2Cg03oIoJib9FBqz5RZaYqGqvwfw+/TrIyLyWwAzAVwI4Kz0bncB2IVUYnEhgLs1NaXYkyIyRUTend73UVU9BADp5OQ8AD8Iq+xE5K+YJgJTBJNqTNx52YdgSKqTWsQETJ8VOJK2i8vv3TNs+y+v+Yh/gUUg0Ky2cIECPlXWUcvM+bQbtYqfDprGRqHxFkRRMekrT8xW+Eo1Y9LHQkRmAzgNwK8AzEgnHVDV34vI9PRuMwEcGHJYV3pbvu1EVEY9fQnc/Gh2++/Nj+7D1z/e6ttEYLuK1w8PDOuUdtK0kXvSFfNH3rZdrN66e9ix231mMWyqi+acebOpFHXdNCYKjbcgwkw8baewmC230EfDikg9gJ0A/kpV3x5p1xzb8qVmw3rciMgqEekQkY7u7u7CCksUkvEYn67r4tJFc/DVB/fi4s1P4qsP7sWli+bADbBiZNJV7yYPpJoWrtqxB0mfznSZP/JDl6gO+kc+30RGftecCDNvjsf4HKrQeAuimJj0U2jMlluoNRYiEkEqqdimqj9Mb35DRN6drq14N4CD6e1dAGYNObwFwOvp7Wcdt33X8ddS1c0ANgNAe3t7Zf/WacKp5PhMJh0cPDoI21VYhmB6fQyRiP/TluOqN3EPkLrhXbNzT6CnKSfPDdNMhfj3AAAgAElEQVQdxR/50ffrMHI+WUZM/+erzMyb41UlxGehcRhEofEWRDEx6aeYmC2n0EqXHuXxXQC/VdV/GvLRAwAuTb++FMCPh2xfISmnA3gr3WTyUwDnikijiDQCODe9jYiKlEw6+N3Bo7h485P4k/W7cPHmJ/G7g0eRTPp3TLTzrGNgBxjAH03fMIca7R/50U6vXV9jYOPytqwny43L21BfU9k36YmgmDgMoph4CyKsKd+rNWbDrLE4E8AlAP5TRJ5Jb/sbADcA2C4inwXwKoBPpD/7CVJDTZ9HarjpZQCgqodE5KsAnkrv95VMR04iKk730UHc+thz2Us+P/YcvvyxU3BC48jrSVsiOHf+dCxum+Udu7PzAKwA4/cVitsvacPBtwe9TmnTJ8WQo5WzZI70Ozm/6/UfOwWTiuy9T8UpJg6DKEe8lUK1xmyYo0L+Hfm7rp6TY38F8Lk85/oegO+VrnREBKQGRFy6aM6wtQiCzO1TEzVw1XnvR9ehVK1F1Ey9DzJNsimCY4M2rvvxb7zr3nzRAkwLsUOk7Soe2XsQj+w9mLX9b8+fH9o1KZhi4jCIcsRbKVRrzHLmTaIJzFXgF/vewB2f/mDW4kyzm+b4Hms7wNv9yayb9S1LT8Xkmkig6165PXup6Su3P4v7V59R9HfKJ5qnvTpa4e3VE0ExcRjo/MgTb2vCi7dSqNaYrezSEVGoIpbgY+nFmc7+xs9x2Z1P4WOntgSaOMh2XXzh3meybtZfuPcZ2EFGhThunt7u/scWKhpJLRY1tL16/ZJWRCPjZ3RHtSomDoPIG29OePFWCtUas6yxIJrAHAferH5A6ma7ZmtnoJoD21EsOqkJKz98kveUueUXL8J2/Nut8y01bZaq7juHgUTudRdu+9RpRS9ERcUpJg6DMGXs460UqjVmmVgQTWC2m/tJLkitQ23UxMoPz0FXb7/XIW7lh+egNuo/RFAEuHFxa2ht6rlELTPnugucQbP8ionDIPJO6V3hU2NXa8xO2MSC63NQJRrr5blNI3cbrmn4t5K6ruJYIns44LGEg8k1/jUWqsBdT7yU9SR21xMv4fqPnTL6LxFQU10Ud3/mQ3il55iXCL2nqZYzaI5CWPFZTBwGokDURNaU3o7rVPqgkKqN2QmbWBBVGtdV/O6/38aqe955qtp8SRve/65JoSUXMVOwYdlCrNu227vmhmULEQvwJOcilUgM7by5fkkrgjxjRkzB58+eO+y6YT9BDtpuVnm3rGgP9XrjSZjxWUwcBio7gEN9SVy1ozMrVifHK/sPNFCdMcvEYgIYTe3MyzecH2JJaCTdRwbx6xffxPdXng5XFYYIHt/7ezTVxTBjcjiD1vttF7c9vj+r5uC2x/fj/waoObDzTJN8b4CZN5OO4qFnXxs2CmDFotKMAsiFy58XJ8z4LCYOgygmVsupWmOWiQVRhbAs4MyTp+OFg0e9as8zT54OK8T/S5084+T/LsA4+WKmSY5ZBs5fMBOX3flU9hOqFd5ANS5/Xpww47OYOAx6/rCm9A5TtcYsEwuiCmHbijePDA5rWpgUC+9/00ie0RlWoLU38hwbpBlFFZPiVlabt2WmtoeFy58XJ8z4LCYOA52/iFgtp2qNWc5jQVQhwlyBMS8BvnnxqVnj5L958amBRmfURXOvY1AXYOZN21H84MlXoOlEQjX1PshQ1UKFuQrlRBBqfBYRh0EUE6vlVK0xyxoLogrhuIrm+lhWO/OmXS+EWl1rGQYipuCrF37Aq96OmBKoN/7RQTfvOgaTfZZ3iEUMXHBcU8jGZQsRi4R3ow9zFcqJIMz4LCYOgygmVsupWmOWiQVRhYhHTFx93jzvqTBT1VxToqWjc0k6Lj73/aeHVbXeF6BTWzHrGIgI6mNmVlOIKQoJecKi8b78eZjCjM9i4jCIal1zA6jOmK3seiCiCSRfz3U7xBoLO0+nNifANa10u/hQQdvFJ8UicBQ4cOgYuo8M4sChY3A0tZ0qU5jxWUwcBlFMrNLoscaCqELYrpuzqrlUsw/mkm+Ro0iARY7qa4ycsxnW1/gf29ufxKfveGrYdSt9GN1EFmZ8FhOHQRQTqzR6TCyIKkTMNPB3F/yBt7BXZrXQWMCbayGzIooAN1+0wFv5MbOcdJAWiSMDLh58pmvYXBSXLJqDyfGRj63WYXQTWbHxOZJi4jCIYmKVRo+JBVGFcBU5VwsNshCT6yr2vXHEm0wn03t83oyGEZOLQdvFP/zkd1lPof/wk9/hlqWn+l7TcRW3/9vLuP3fXs7avuz02b7HVuswuomsmPj0U0wcBlFMrNLoMbEgqhCJIpZ27ulL4FcvdA+bFXFafWzEpoWoaeRc5ChIFXTMyl19HQ0wyVVmGN3xiVClD6ObyIqJTz/FxGEQxcQqjR5/q0QVwszTwSzI0DLTUJw9/11I2C5cV5GwXZw9/10wjZE7v9XGBLdfkj2+//ZL2lAb87+mIYJblmbPPXDL0lNhBKi/HjqM7pfXfAQ/Wnemb+0KlVcx8emnmDgMophYpdFjjQVRhYgagvVLWocN54sGuHErgMPHksMWcaqLjdy08Ha/g9d7j+G+VafDdhWWIdj7+luYVGP5tj0P2g6+9uBvs6qvv/bgb/GtTwarvq7GYXQTWTHx6aeYOAyi2Fil0WFiQVQhDEPQVB/NmiSoqT7YZDgDCddLKoBUFfW6bbtT8wDU5T/OFMHfP/jbYVXE2wPMH2AauauvS7bUNVWUYuLTTzFxGOj8jNUxxd8qUYXoSzj4zi9eQktjHM0NMbQ0xvGdX7yEYwn/kRJ2elbE2y9pw32rTsftl7ShuT7mO8dATdTEzRctyKoivvmiBaiJ+neinF4fw6bjpknetLwN0+tZCzEe5YvPvgDx6aeYOAyCsTq2WGNBVCFMQ/DEiz3Y3tnlbWtpjOOKj871PbbGMnLPiujTOW1KPIqm+thxT6ExTIn7d6K0LAPvn9GA7avPgO24sEwD0+tjsNghblyK5InPvwoQn36KicMgGKtji79VogoRSbdhD32qWr+kFZEAVc2uIuesiH4TFxqGYHZTHT4wczJaGuP4wMzJmN1UF7h627IMnDAljhOb6nDClDhv1OOYZRo549MqwciNYuMwCMbq2GGNBVGFMA3BtOOe2qbVx2AGuLkmixgKyE6UFMRA0sFND+/L6gB508P7SjbXBONw/GBiQVQhbFchopg1tdZbmAtwA63FYBqCc+dPx+K2Wd5Nf2fngUBJCVEQpiFobshummhuiDLGaBgmFkQVwjQEfYMODvUNeDUWU+simBygnbnGMnDFOSdjzZC1EDYtb/PtY0EUFGOMgqqaxEJEzgNwCwATwHdU9YYyF2lcmn3tQ6Pa/+Ubzg+pJBNPYzyKt/qTONSX9LbVxSw0BkgsBh3Xu+EDqWaQNVs7sX11aYbrETHGKKiqSCxExATwbQB/CqALwFMi8oCq7i1vyYhKx7IMzJ5ah9qoNeqe60kn97LTthPekus0sTDGKKhqqcP6EIDnVfVFVU0AuBfAhWUuE1HJFdpzPczplokAxhgFVy2JxUwAB4a870pv84jIKhHpEJGO7u7uMS0ckZ+w47OYoapEQeKTMUZBVUVTCIBckZtV/6aqmwFsBoD29nbWzVFFCTs+o5ZgWsNxQ1UbYohavOmTvyDxyRijoKolsegCMGvI+xYAr5epLEQVJ25aqI+5eN/0em/ZdMtMbScqBcYYBVUtTSFPAZgrInNEJApgKYAHylwmoopRU2OhMRZFplbaEKAxFkVNDW/6VBqMMQqqKiJCVW0R+TyAnyI13PR7qvpfZS4WUUWpqbEwkzd5ChFjjIKomghR1Z8A+Em5y0FERET5VU1iQZWJE2oREdFQ1dLHgoiIiKoAayxoTI2mhoO1G0RE1YeJBVUsNrMQEVUfUR1/c0mJSDeAV3x2mwbgzTEoTqEquXzVXLY3VfW8sSpMLlUcn5VWpkorD1B8mRif4avWsldCuQPF57hMLIIQkQ5VbS93OfKp5PKxbOGrxO9RaWWqtPIAlVmmMFTz96zWsldTudl5k4iIiEqGiQURERGVzEROLDaXuwA+Krl8LFv4KvF7VFqZKq08QGWWKQzV/D2rtexVU+4J28eCiIiISm8i11gQERFRiTGxICIiopJhYkFEREQlw8SCiIiISoaJBREREZUMEwsiIiIqGSYWREREVDJMLIiIiKhkmFgQERFRyTCxICIiopJhYkFEREQlw8SCiIiISoaJBREREZUMEwsiIiIqmXGZWJx33nkKgD/8yfVTdoxP/ozwU3aMT/6M8BPIuEws3nzzzXIXgSgvxidVMsYnFWtcJhZERERUHkwsiIiIqGSYWBAREVHJMLEgIiKikmFiQURERCVjlbsAQYnIlQD+EqkhL/8J4DJVHShvqcrHdRU9fQkkbAfxqAnbVSRtF1HLRGM8gt7+JBK2k/N9U10UhiFZ53qzbxCO68J1AVcVMcsEBLAdF6qA4yqilgFDBP1JB1HTQMQUDNouHFdhGoKIKUg6CkcVliEwRaAABIDtKuzMfobAUQUUsEwDA0kHliEwDIGrClXANASuAqqKiGUgagr6Bh0YBqAq3nbLEPQnHIgITAEMwxj2/aiyzb72ocD7vnzD+SGWhGjiGRiw0dOfgO2m7ttN8ShqaopLDaoisRCRmQCuADBfVftFZDuApQDuLGvBysR1FfveOIKVd3eguT6Gq8+bh6t27EFXbz9aGuPYtLwN33rsOTyy9yDOnT8dV5xzMtZs7fQ+37KiHfNmNKT+kKfPdfOj+3Dpojm4Zuc75/n2p07DQNLFl+5/1tu2fkkrbnp4H5oborj87LlYu203unr7ce786fj82XOxLv2+pTGOmy9agIa4hSP9Nq7cnn2OeNTEhp89j8vOnIObHt6H7qODuPmiBYhYBjb87PmssmTOfdvj+4eVMVOe7qODuHFxK+564iVc+afzvO9HRES5DQzY2N/Th7VD/j5sXN6GuU11RSUX1dQUYgGIi4gFoBbA62UuT9n09CWw8u4OdPX2Y81Z7/WSCgCpbVs7sbhtFgBgcdssL6nIfL7y7g709CWyzrW4bZb3Bzuz36G+pJdUZLZdtWMP1pz1Xixum+UlFZnrrBvyvqu3H1dufxaWYXpJxdBz9PYlsbhtlne+zP6Z7UPLkjl3rjIOPf6anXuwuG1W1vcjIqLcevoTXlIBpO6pa7d2oqe/uPtnVSQWqvoagH8E8CqA3wN4S1UfGbqPiKwSkQ4R6eju7i5HMcdMwna8QJgSj3ivM7p6+zElHhnx84TtZJ0r1361UTPvuY/fP991DEHO7bVR0zsmU9bjtx9/br/vOnSfzPerFBMpPqn6MD4nJtvVnPdU2w08yWZOVZFYiEgjgAsBzAFwAoA6EVk+dB9V3ayq7ara3tzcXI5ijpmoZaKlMQ4AONyf9F5ntDTGcbg/OeLnUcvMOleu/Y4lnLznPn7/fNdxFTm3H0s43jGZsh6//fhz+33Xoftkvl+lmEjxSdWH8TkxWYbkvKdaRTYjV0ViAeCjAF5S1W5VTQL4IYBFZS5T2TTVRbFlRXuqP8WuF7B+SasXHJk+Fjs7DwAAdnYewKblbVmfb1nRjqa6aNa5dnYewI2Ls88ztS6Cb3xiQda29UtasWnXC9jZeQAbly30PtvZeQAbhrzP9LGwXQc3XzT8HI11EezsPOCdL7N/ZvvQsmTOnauMQ4+/cXErdnYeyPp+RESUW1M8io3H/X3YuLwNTfHi7p+iWlyVx1gQkT8C8D0AHwTQj1SnzQ5VvTXX/u3t7drR0TF2BSyDcEaFKFxX4SoQswxAAMdx4ZZoVIjjKoyiR4UIdMj2AkaFlL1H50SIz9HgqJAsjE8aU6McFRIoPqtiVIiq/kpEdgDYDcAG8DSAzeUtVXkZhqC5IZb38+M/G2lfwxBMb6gpWdnCMqU2zwd1Y1oMIqJxo6bGwswih5ceryoSCwBQ1esBXF/uchAREVF+1dLHgoiIiKoAEwsiIiIqGSYWREREVDJMLIiIiKhkmFgQERFRyTCxICIiopJhYkFEREQlUzXzWIw3Q2fOzDUbZoZtuzh4dBCAQiBIOC5MQxA10rNaCmC7QDK9PWIIaqIGBhIuRASqiqSrUFXETCM1Q2d6hrV41IDtaGq2TFdhmanZMgdsF5YhqI0a6E+4gABR08Cg7Xqzs0VMgWUKBhIukq4iZhlwFbDT5YhZBpKuixrLwLHEkOMsAVQQiwiODjje9phlwAW8mTRH+p0QEU0Uo5wZsyJUdunGKddV7HvjiLf0eWb9jnkzGrL+kNq2i9+9cQTfeuw5rPmT9+GKe5/29l+/pBXNDVEMJNVbFj2zfVpDDDWWgbf7k3h7wMZVO/aguT6Gq8+b5y2xnpkTfmqdhb//l714ZO9B7/ibHt6H5oYoLj/nZNz62HP44rknI2Grtyx6S2McG5ctxOTaCD615Vc5z71h2UJMrYugq3fAW149s/2VN49gdvMkb7nezPaHnn0NZ71/Bm56eB+6jw7m/J0QEU0UAwM29vf0Zd0rNy5vw9ymuopOLtgUUgY9fQkvqQBSy9SuvLsDPX2JrP0OHh3Emq2dWNw2y0sqMvtftWMPAMNLKoZu7zrUD0Bw8EjC+2O/5qz3eq8z+67d2gnXFSxum5V1/Jqz3ovFbbOwNn1tyzC9pMI7dttuJGzNe+5123YDEC+pGLr9tPc0ef+jDN2+pP1E7/r5fidERBNFT39i2L1y7dZO9PRX9n2xclOecSxhO16gZHT19iNhO1nbko6Lrt5+TIlHcu5vCHJur42acFRRGzW9z/Odw1XFlHgka1vmfeZ1vutkKhLyndtxdVTbTUOGXf/43wkR0URh57lX2m5lLx7KGosyiFqmt0xtRktjHFHLzNoWMQ20NMZxuD+Zc39XkXP7sYQDUwTHEo73eb5zGCI43J/M2na4P+ntf7g/mfc6mdjOd27TkFFtd1z1rpnvd0JENFFYee6VVoU3DzOxKIOmuii2rGj3AibTx6KpLpq13/T6GDYtb8POzgP41tLTsvZfv6QVgItNy9uGbW+ZGgegmN4QTb1vjGPTrhe815l9Ny5vg2EodnYeyDp+064XsLPzADamr227DjYsW5h97LKFiFqS99wbli0EoNh43HEbli3E06/0YONx5d6wbCF2dLzqXT/f74SIaKJoikeH3Ss3Lm9DU7yy74uiWtlVKoVob2/Xjo6OchdjRKMdFZIeA4Kk48LIMSrETm8v+6gQ14UpmVEhihpLKm1USNlT/WqIz6FmX/vQqPZ/+YbzQzv/aM9dhRiflKXCRoUEik/2sSgTwxA0N8R897MsAydMifvud7zJoz8kp8Y6nx38Ph/hHHnLGOCcREQTQU2NhZkVPAIkFzaFEBERUclUVxpERBNa2M0yRFQ81lgQERFRyTCxICIiopJhYkFEREQlw8SCiIiISoaJBREREZUMEwsiIiIqmaoZbioiUwB8B8AHACiAz6jqf5S3VLln0ATgbYuYBmwnNTtlxDQQswQDSReOqzDTs2QOJlyIAK4CRvpfJ72/IYCjClXAVYUh4u0bswwkbReWKUg66s3MVhcz0Df4zmyX8fQMmrariKRn14wOmRGzJj1rZsJxYaZn73RV4WhqJkxBaoZP1dSiOPZxM20akpqlMzMLZ8Q0ML0+BssyvJlDk46btZ2IiPxV2MybgVR26bLdAuBhVV0iIlEAteUukOsq9r1xxFsCPbO+RcwysOJ7v0ZzfQxXnzfPW1L83PnTcfnZc72lxM+dPx1XnHMyvvXYc7h00Rzc9cRLuHTRHFyzc493vpsvWoCIZeDz33/a23bj4lbc9cRL+PzZc7H75R4snN3kLWu++n/MxgWntnhL7Z47fzouP+dk731LYxy3fvI01NdYuOyOp9BcH8Pf/Pn7ceX2Z73P1y9pRTxqYsPPnsdlZ85BbdREbdREwnaxeuvuYd8rs9bHbY/vxyN7D6bWD1nehpOb6/Bcd5+3tHtm+/tnNDC5ICLyMTBgY39PX9b9e+PyNsxtqqvo5KIq7u4iMgnAhwF8FwBUNaGqh8tbqlStRCapAFLL2a68uwOv9BxDV28/1pz1Xu+PLwAsbpvlJRWZ92u2dmJx2yxcs3OP9+/Q8125/Vn09iWztmX2XbdtN86e/24vqQCAJe0nekHoXXPI+67eflz+g6fRdajfK2Mmqch8ftWOPejtS2Jx2yxctWMPDvUl0dU7gINHEjm/V1dvP9Zt243FbbO892u2dqK7L+ElFUO3Hzw6GO5/GCKicaCnPzHs/r12ayd6+hNlLtnIqiKxAHASgG4Ad4jI0yLyHRHJWlFCRFaJSIeIdHR3d49JoRK24/0Hz+jq7UdtNLXU95R4JOvzfO+P/zff+YZuy+zrqmYdYxoy4jX9yjj088xnmRoLv2OmxCNZ721Xc+5nOy4mmnLEJ1FQjM/KlPce6lb24qHVklhYABYC2KiqpwHoA3Dt0B1UdbOqtqtqe3Nz85gUKmqZ3nK2GS2NcRxLOACAw/3JrM/zvT/+33znG7ots68hknWM4+qI1/Qr49DPM58dSzjez0jHHO5PZr23DMm5n2VWS9iVTjnikygoxmdlynsPLf2qzyVVLXf4LgBdqvqr9PsdSCUaZdVUF8WWFe3ef/hMH4v3NNWm+hPsegHrl7R6n+/sPICNyxZmvd+0vA07Ow/gxsWt3r9Dz3fzRQvQWBfJ2pbZd8OyhXh87++xYcg5d3S8io3L27KvOeR9po9Fy9S4V8abL1qQ9fn6Ja1orItgZ+cBrF/Siql1EbQ01mB6QzTn98r0sdjZecB7v2l5G5rroth03LU3LW/D9Hr/VV2JiCa6pnh02P174/I2NMWjZS7ZyES1sqtUMkTk3wD8paruE5EvA6hT1aty7dve3q4dHR1jUq6go0JsV2ENHRWiClNGGBWiioiRe1RIatvoR4U4rsLKjAqJCI4NFjYqxHEV0VGOCrEdF1ZljAope6o/lvFZCmEv/DXa849GFS5CxvikLBU2KiRQfFZut9LhLgewLT0i5EUAl5W5PAAAwxA0Nwx/As+1La86/11Ga8rxY2ZyXKOx1ONqclzDsgycMCU+/AMiIvJVU2NhZgWPAMmlakqrqs8AaC93OYiIiCi/auljQURERFWAiQURERGVDBMLIiIiKhkmFkRERFQy5R33J9IoIq3lLAMRERGVzpgnFiKyS0QmichUAM8iNU33P411OYiIiKj0ylFjMVlV3wbwvwHcoaptAD5ahnIQERFRiZVjHgtLRN4N4CIAf1uG65fd0Nk6I5aBuggwYAODSRdiAK6bWvPDSM+CaRqChOMiHjFwLD2DpmmINxtmZvbOZHpmNssQDNoujPRrARCLCBJ2ZgbP1CybMctAbVRwZOCdWTqjlgHHVURMQWNtDEaFz0lPRFRuFTY7ZtmV45t/BcBPAfy7qj4lIicB2F+GcpSF6yr2vXHEW2791qWtWHDiVLzVb+PWx57DpYvmeEunZ9btmFYfRX2Nha7Dg94Supk1Q36x7w187NQWb3nyzDE3PbwP3UcHsX5Ja2pl0piFyTUm3jyawOqtu719Ny1vw7ceew6P7D3orfmRmXb8rWM2Zk+rY3JBRJTHwICN/T19WffmjcvbMLepbsImF2PeFKKq96tqq6quS79/UVUXj3U5yqWnL+ElFQBw2nuakLAVa7d2YnHbLC+pAFLL4161Yw+6egfguPACN/PZNTv3YEn7iV5SMfSYNWe913t9qC+JrkP9SDjAwSOJrH3XpK+beb9u226YholDfUm8cugYevoSY/0rIiKqGj39iWH35rVbO9HTP3HvnWOeTonIHKTW/Zg99Pqq+hdjXZZySNiOF4BAuslDUsE4JR7J+gxIba+NmnBczfmZaUjO7VPikazjM9fKvM61b+a9IfD2S9jZS7YTEdE77Dz3ZtutjgU+w1COepp/BvBdAP8CwC3D9csqaploaYx7gWgaqZVJWxrjONyfzPoMSC2TeyzhwDQk52eOqzm3H+5PZh2fuVbmda59M+9dhbdf1MpORIiI6B1WnnuzNYGbkMsxKmRAVb+lqj9T1Z9nfspQjrJoqotiy4p2tDSmVvx8+pUeRC3BxuVt2Nl5ADcubvU+y/SXaGmsgWkAG5e3ZX124+JW7Oh4FZuO275+SSs27XrBez21LoKWqXFETWB6QzRr303p62beb1i2EI7rYGpdBO+ZWustA09ERMM1xaPD7s0bl7ehKT5x752iOrbVNSLyKQBzATwCYDCzXVV3l+oa7e3t2tHRUarTlVzRo0JUYcrwUSF2erRIkFEhScdF1DJQGxMc6Z9Qo0LK/oUqIT5nX/tQaOd++YbzR7V/JZWlAjA+q9AEGhUSKD7L8c3/EMAlAM7GO00hmn4/IRiGoLkhlrWtLuCxjfl2DHqCHCbVFH4sEdFEV1NjYeb4TCQKUo7fxMcBnKSqE7fLLBER0ThVjj4WzwKYUobrEhERUcjKUWMxA8DvROQpZPexmBDDTYmIiMazciQW15fhmkRERDQGxjyxUNWfi8gMAB9Mb/q1qh4c63IQERFR6ZVj2fSLAPwawCeQWojsVyKyZKzLQURERKVXjqaQvwXwwUwthYg0A/h/AHaUoSxERERUQuUYFWIc1/TRU6ZyEBERUYmVo8biYRH5KYAfpN9fDOAnQQ4UERNAB4DXVPWCkMqX09DZMqOWicZ4BH3JJI4Nukg4LibHTfQNuoiYqbU/MrNipmaxNGAIIAZg24pkeobMqGnANICB5DszX9ZEDQwk3nkfMQWOAoBCVRCzBP1JF64qaiImptWNy9kxiYhKagLNjll25ei8eZWI/G8Af4zU9KCbVfVHAQ//AoDfApgUVvlycV3FvjeOeMudtzTGcf+a09FzNIk1WztxxUfeiz+YOQUPPtOF8xfMxG2P78eliyDhf20AACAASURBVOZ4S6C3NMZxy9JTManGwmV3vnOOb3/qNJiG4S17nplj/sFnunD7v72cer9sIR589jX8WesJ+Nc9r+P8BTOxbttub/8tK9oxb0YDkwsiojwGBmzs7+nzljfP3GvnNtUxuQhBuZogfgngZwAeS7/2JSItAM4H8J0Qy5VTT1/CSyqA9JK4DryEYNHcZqzd2okl7Sdi3bbdWNw2y0sqMvt/4d5n0NU7kLXtUF/SO0dmW+Y83vttu7Gk/UR8/vtPe+cfuv/KuzvQ08dJTImI8unpT3hJBfDOvbann/fOMIx5qpYeFbIewC6kaixuFZGrVNWv8+Y3AVwNoCHPeVcBWAUAJ554YsnKCwAJ28laEhcAHFVvm+OmXpuGoKu3H1PikWH7d/X2ozaavQR5bdTMuZ85pPZh6Hkz/x6/f8LOXgqdKk+Y8Un5jXaBsypctKwkxnt82q7mvHfa7tguwjlRlKPGIjMq5FJVXQHgQwCuG+kAEbkAwEFV7cy3j6puVtV2VW1vbm4uaYGjluktiZthinjbTCP12nEVLY1xHO5PDtu/pTGOY4nsBOBYwsm5nzMk2IeeN/Pv8ftHreyEhSpPmPFJVKzxHp+WITnvnRabkENRLaNCzgTwFyLyMoB7AZwtIltDKt8wTXVRbFnR7gVmS2MclglsWt6GlsY4ntjfjY3L27Cj41VsWLYQOzsP4MbFrVn737L0VLQ01mRtm1oX8c6R2ZY5j/d+2ULs6HgVt33qNO/8Q/ffsqIdTXXRsfpVEBFVnaZ4FBtz3Gub4rx3hkFUx7YqSETWA2hF9qiQPap6TcDjzwLwf0YaFdLe3q4dHR3FFjXLSKNCko6LSflGhagiYowwKsRE1iiQzKgQJ71PxBI4LsBRISVT9l9WGPE5WqNtIhiN0TYnhFmW0aqAphDGZ0g4KqQkAsVntY0KKRvDEDQ3xLK2TbZimDykdm1KbYEnr/N5T0RERampsTCTicSYGNPfcnoeip+q6kcB/LCQc6jqLqQ6fhIREVGFGdM+FqrqADgmIpPH8rpEREQ0NspRLzQA4D9F5FEAfZmNqnpFGcpCREREJVSOxOKh9A8RERGNM+XovHnXWF+TiIiIxsaYJRYi8p8A8o5tVdXWsSoLERERhWMsaywy8058Lv3vPel/lwE4NoblICIiopCMWWKhqq8AgIicqapnDvnoWhH5JYCvjFVZiIiIKBzl6LxZJyJ/rKr/DgAisggVOiWUbbvo7huE7bioj5k4lp4hsz5mYiCZel1jGXAVsF0Xhog342ZdzMDb/Q4sU2AZqVG9A0kHliEQAzANg7NmEhGNEc68OXbK8Vv9LIDvpeeyUABvAfhMGcoxItt2se+NI1i9tRMXt7XgrD+YgbVbO7HopCYsP+M9WLdtN5rrY7j6vHm4akdqifSWxjhuXNyKu554CZefczJ++9phfOtnL+CWpafCMgRffmAvuo8O4hufWIDv/vuLuPJP52HejAYmF0REIRoYsLG/p89bOj2zVsjcpjomFyEY80XIVLVTVRcgtV7Iqap6qqruHuty+Dl4dBCr00F44cIWLyBXfvgkrNu2G129/Vhz1nu9pAJILcN7zc49WNw2K5WEzG1GV28/vnDvMzjUl8Sas96Lrt5+fOn+Z7G4bRZW3t2Bnr5Emb8pEdH41tOf8O7hQOpevXZrJ3r6ef8Nw5gnFiIyQ0S+C+A+VX1LROaLyGfHuhx+ko7rBaGr6r02DfFeT4lHvNcZXb393vbM8uddvf2ojZqYEo8M2ydhZy+lTkREpWW7mvNebbtjuwjnRFGOZdPvBPBTACek3z8H4K/KUI4RRUzDW2LXEPFeO656rw/3J73XGS2NcW+7mW7iaGmM41jCweH+5LB9opY5Vl+JiGhCsgzJea+22AwdinIkFtNUdTuA1GLgqjaAintsn14fw+3L29DSGMePd3dhY/r1ll+8iA3LFqKlMY5Nu17A+iWtXsBm+ljs7DyAjcvb8MT+brQ0xnHL0lMxtS6CTbteQEtjHN/4xALs7DyALSva0VQXLfM3JSIa35riUe8eDsDrY9EU5/03DOXotdInIk1IT5YlIqcj1YGzoliWgXkzGnD/mjO8USH3rTrdGxWSeV1jGbh/9RlZo0Ku/9gpqIsZmFwzDX98crM3KuSWT54KSwRiCL728T/kqBAiojFQU2NhblOdd9/mqJBwleO3+kUADwA4KT1/RTOAJWUohy/LMvDuye9UnzWOclDslNoSF4iIiApSU2NhJhOJMVGO3/JeAD9CarbNIwD+Gal+FkRERFTlytHH4m4A7wfwDwBuBTAX70zvTURERFWsHDUW89LzWGT8TESeLUM5iIiIqMTKUWPxdLrDJgBARP4IwC/LUA4iIiIqsXIsmx4BsEJEXk2/fw9S/S6IiIioypVj2XQiIiIap8Z82XQiIiIav8rRx4KIiIjGqaqYLUREZiE1TPVdSE0FvllVbwnzmq6rONyfwKDtImqmZtTsT7gQAaCpRW1MI7U9YhpI2K43E6erqUXMopYBwwCStsIwAHWB5voYIhGuD0JENNTAgI2e/gRnxhwHquW/mg3gS6q6W0QaAHSKyKOqGkqnT9dVvNzThyMDNp5+pQfn/uG70XM0iVsfew6f/eOT8KX7n0VXb39qvvllCwEAa7ftRnN9DFefN89bSr2lMY7bL2nDA0934cPzZuCuJ17C5eecjPdPr2dyQUSUNjBgY39Pn7e0eWYtj7lNdUwuqlBVNIWo6u9VdXf69REAvwUwM6zr9fQl8ErPMXzu+7tx9vx3w3GAtVs7sbhtlpdUAKlld988msDabbvR1duPNWe910sqMp+vvqcTS9pPxDU792Bx2yys3dqJg0cHwyo6EVHV6elPeEkFkLp3rt3aiZ7+RJlLRoWoisRiKBGZDeA0AL86bvsqEekQkY7u7u6irpGwHdRGTXT19sNVhaOKrt5+TIlHvMDPyOwH/P/27j46rru+9/37u2c0mrHsYFlxQrDsOOQ44YZeO1i6nJBwuG64paHhoXdZhBSLBO5dMXZ4bhug59zTcrp6zyL40jRQIpMAhwSHhmCXUwgpkAW4aUt5kJzEgUCewIkV0sRRZOKHkUYz+3v/mD2TGXkky9aeJ+vzWmuW9v7t3977O1vf2fpqz36g5vTR8SyJwKrmz4c+r/ik/cSZnyJxa3Z+5kOvue/UvrI9tVVhYWaLgV3Ah9z9hcpp7n6zu/e7e//y5cvntZ5UMsHRXIHe7gyBGQkzerszHMxOlR+7W1LqB9Sc3tudoRB61fxJPdF0wYkzP0Xi1uz8TAZWc9+pfWV7apvCwsw6KBYVt7v739dzXT1dKc7uWcRn37Ge7z/0NIkEDA32sWtkP59627ryB6C3O8Ppi1MMbVpPb3eG7bsfZ9vA2qrpn3tnHzuHn+T6jWvZNbKfocE+zljcWc/wRUTaSk8mxdBgX9W+c2iwj55MqsmRycloi7NizMyALwC/cPe/rvf6gsBY3dPFwWyOM057GamEsWJpJ3/x5lcSGNy5+SLy7iTMCAySiYCvbr6ofFXI197zGqbCkFSieFXIVRefQxDAx9/8Sl0VIiIyTTqdZE1PV3k/qqtC2lu7/NYuAd4JPGhm90dt/9nd767XCoPAWNY17chCV73WJjI/qz/2rWaHIDIv6XSSFSokTglt8Vt0938B9GWbiIhIi2ubcyxERESk9amwEBERkdiosBAREZHYqLAQERGR2KiwEBERkdiosBAREZHYqLAQERGR2LTFfSziNDGRJ0+eoznIFUIKodMRGJlUQHYqJF9wkomAwGAyH5IMjI5EQCppnJZOEeje9SKyQE1M5BnL5nR3TJnVgsqIiYk8OfI8+8IUBw5Nlh9x3tudYftgH9+8f5TP/fM+erszbBtYyye//TAHDk9ywxXr6O5KcWgiz4qli1RciMiCMzGR59GxI+XHm5ee57Gmp0vFhVRZUF+FjGVzHMqG7H8+Wy4qoPh43i07RhjoX1Uev27nXrZsOJfR8SwfvvMB9j+fZTLvjB3JNfMtiIg0xVg2Vy4qoLif3LpjhLGs9olSbUGVmfnQAViUSpQ/HCWj41kSFUciRsezLM10lIcXpRIEBrl8oXEBi4i0iHzoNfebpf2qSMmCKiySUeFwNFegtztT9SHp7c5QqPiA9HZnOJidKg8fzRUIHVJJPZlU6k8PFZNWkwys5n4zqa+GZZoF9VVITybFkkzAymXFcyh6uzMA5XMsdg4/WR7fNrCW7bsfp7c7ww1XrGPlsgydSaOnK9XMtyAi0hQ9mRRDg31V+82hwT56MtonSrUFdcQinU7CBLz0NDgt3cEdmy+quirk6kvOYdNFq8tXhdx45YUkdFWIiAjpdJI1PV18dfNFuipEZrXgMqL4IUiyOH3stO6GRyMi0j7S6SQrVEjIcSyor0JERESkvlRYiIiISGxUWIiIiEhsVFiIiIhIbFRYiIiISGxUWIiIiEhsdN2QiEjkRO54uu8Tl9cxEpH2pSMWIiIiEpu2OWJhZpcBNwIJ4PPu/om4lp3Ph7wwmSObC0l3BOQLzlTopBKGO0yFTiF0UokAM5jMh6QSAQmDbD4s3p0zMDqSRlcySWdn22xWEZE5m5jIM5bN6c6bMqu2yAgzSwCfBX4PGAV+ambfcPeH5rvsfD7kNy9kOXh0ij37xrjkvDN47tAku3/5DBv7VzJ2OFd+xHpvd4Ybr7yQv7rrFxw4PMkNV6zjv9/9Sw4cnmTbwFpOX9LJRGdID6i4EJFTysREnkfHjpQfnV56Vsiani4VF1KlXb4KeTXwmLv/yt1zwB3AW+NY8LOHJ8nlnWtv38OlF5zF6PNZrtu5l4H+VTw1PlEuKqD4iOAP3nE/Wzacy+h4lg/f+UB5+Lqdexl9PkuhAM8dzcURmohIyxjL5spFBRT3h1t3jDCW1f5OqrVLYbEC2F8xPhq1lZnZZjMbNrPhAwcOzHnBU4WQwIofktCdRakEo+NZEoGVhyuNjmdZmumoObwolSB0J1/x+HUROPn8FGmEueRnPvSa+0Pt72S6diksaj1StCqb3f1md+939/7ly5fPecEdiYDQi48ADsw4mivQ252hEHp5uFJvd4aD2amaw0dzBQIzknoCqkxzsvkp0ghzyc9kYDX3h9rfyXTtUliMAisrxnuB38Sx4DMWd5JKGjdtWs/3H3qa3mUZtg2sZefwk6zoTrNtYG35w1Q6x2L77sfp7c5wwxXrysPbBtbSuyxDIgGnL0rFEZqISMvoyaQYGuyr2h8ODfbRk9H+Tqq1yxk3PwXWmNk5wFPAlcA74lhwMhnwstMyLO5M0NN1FumOgNM6k6y8+BxSCeMl6Q7u2HwRYeh0RFeF3HjlhXREV4XceOWFBLoqREROcel0kjU9XXx180W6KkRm1RYZ4e55M3sf8B2Kl5t+0d1/Htfyk8mAZck0dMW1RBE51Z3IzbTg1LihVjqdZIUKCTmOtskQd78buLvZcYiIiMjM2uUcCxEREWkDKixEREQkNiosREREJDYqLERERCQ25n7q3TXNzA4ATxyn2+nAcw0I52S1cnztHNtz7n5Zo4KppY3zs9ViarV4YP4xKT/rr11jb4W455Sfp2RhMRdmNuzu/c2OYyatHJ9iq79WfB+tFlOrxQOtGVM9tPP7bNfY2ylufRUiIiIisVFhISIiIrFZyIXFzc0O4DhaOT7FVn+t+D5aLaZWiwdaM6Z6aOf32a6xt03cC/YcCxEREYnfQj5iISIiIjFTYSEiIiKxUWEhIiIisVFhISIiIrFRYSEiIiKxUWEhIiIisVFhISIiIrFRYSEiIiKxUWEhIiIisVFhISIiIrFRYSEiIiKxUWEhIiIisVFhISIiIrFRYSEiIiKxUWEhIiIisTklC4vLLrvMAb30qvVqOuWnXrO8mk75qdcsrzk5JQuL5557rtkhiMxI+SmtTPkp83VKFhYiIiLSHCosREREJDYqLERERCQ2KixEREQkNslmBzBXZrYPOAQUgLy79zc3ouYKQ2fsSI5cvkAmlSAfOlP5kFQyQXemg/HsFLl8oeZ4T1eKILCqZT13ZJJCGBKGELrTmUyAQb4Q4g6F0EklAwIzslMFUomAjoQxmQ8phE4iMDoSxlTBKbiTDIyEGQ4YkA+dfKlfYBTcwSGZCJiYKpAMjCAwQnfcIREYoYO705EMSCWMI5MFggDcrdyeDIxsroCZkTAIguCY9yfSTBMTecayOfJh8XPRk0mRTrfNrlfkhLVbdv+uuy/4U5bD0Hn4mUNcc9swyxd38pHLzue6nXsZHc/S251h+2Afn/7eI3z3oWd5wwVn8IHXn8eWHSPl6bdc1c/5Zy4p/iGPlnXDPQ9z9cXn8NFdLy7ns+94FRNTIX/ytQfKbdsG1vLJbz/M8iUp3n/pGrbevofR8SxvuOAM3nfpGq6Nxnu7M9xwxTqWZJIcyub58J3Vy8ikEtz0g8d49yXn8MlvP8yBw5PccMU6OpIBN/3gsapYSsv+2+8/ekyMpXgOHJ7k+o1rufWHv+bDv3d++f2JNNPERJ5Hx46wteLzNzTYx5qeLhUXTbL6Y9+ac999n7i8jpGcuvRVSBsaO5LjmtuGGR3PsmXDueWiAii27RhhY99KADb2rSwXFaXp19w2zNiRXNWyNvatLP/BLvV7/shUuagotV23cy9bNpzLxr6V5aKitJ5rK8ZHx7N8+M4HSAaJclFRuYzxI1Ns7FtZXl6pf6m9MpbSsmvFWDn/R3ftZWPfyqr3J9JMY9lcuaiAYs5u3THCWFb5KaeudiosHPiumY2Y2ebpE81ss5kNm9nwgQMHmhBe4+TyhfKOammmozxcMjqeZWmmY9bpuXyhalm1+i1KJWZc9vT+M60nMGq2L0olyvOUYp3ePn3Zx3uvlX1K769VLKT8lBflQ6+Zs/lwzvcaagjlp8SpnQqLS9x9PfBG4L1m9rrKie5+s7v3u3v/8uXLmxNhg6SSCXq7MwAczE6Vh0t6uzMczE7NOj2VTFQtq1a/o7nCjMue3n+m9YROzfajuUJ5nlKs09unL/t477WyT+n9tYqFlJ/yomRgNXM22WJf0yk/JU5tU1i4+2+in88CXwde3dyImqenK8UtV/UXz6fY/TjbBtaWd16lcyx2jewHYNfIfrYP9lVNv+Wqfnq6UlXL2jWyn+s3Vi9nWVcHn3rbuqq2bQNr2b77cXaN7Gdo0/rytF0j+7mpYrx0jkU+LHDDFccuo7urg10j+8vLK/UvtVfGUlp2rRgr579+41p2jeyven8izdSTSTE07fM3NNhHT0b5Kacuc2+tQ3K1mFkXELj7oWj4HuAv3f3btfr39/f78PBwQ2NstPpcFeKEoRM6dCYDMCgUQsKYrgophE4w76tCDK9oP4mrQpr+r+JCyE950QleFaL8rDOdvDkvc8rPdjkt+Uzg62YGxZi/MlNRsVAEgbF8SeeM06dPm61vEBhnLEnHFlu9LF00w4SuhoYhckLS6SQrdAWILCBtke3u/itgXbPjEBERkdm1zTkWIiIi0vpUWIiIiEhsVFiIiIhIbFRYiIiISGxUWIiIiEhsVFiIiIhIbFRYiIiISGxUWIiIiEhsVFiIiIhIbFRYiIiISGxUWIiIiEhsVFiIiIhIbFRYiIiISGxUWIiIiEhsVFiIiIhIbFRYiIiISGzaqrAws4SZ3WdmdzU7FhERETlWstkBnKAPAr8ATmt2ICVh6IwdyZHLF0glE/R0pQDKbR2JgHwhZCp0OhIBnUljYiqkEDqJwEinAiZzIWYQOgTRz0LUPzAouOMOoTuBWblvZzJgKh+STBhTBScfOsnA6OoMODIZlsczqYBs7sV1moFhBAYT+ZB0MiB0yBVCEoHRERihOwWHZGAYkEoaR3PVy5yYcvKFkCAwFkXrKE1flAqYyId0JqvbO5IGbnR2GIcnCuX2zmRASHF92dyL2zIIrKm/Xzm1TEzkGcvmynnXk0mRTh9/N3iy84ksRG3zyTCzXuBy4P8F/rjJ4QDFouLhZw5xzW3DjI5n6e3OcMtV/XQmA6764k9YvriTj1x2Ptft3MvoeJY3XHAG7790DVtv31Me/8Drz+PT33uEqy8+h1t/+GuuvvgcPrprb3l5N1yxjo5kwPu+cl+57fqNa7n1h7/mfZeuYc++Mdav7uHaaJnv+U+redOFvWzdMfLiOl9/Xnm8cv53X3IOX9/zFG/r7+XDdz5Qnr5tYC2ZVIKbfvAY777kHJZ1dTBVgC0Vyxga7OMz33uE7z707IzjZy7p4KnxifL77e3OcNOm9Tzx3CFWLz+tKqabNq3nWw88xYZXnMknv/0wBw5PcstV/Zx/5hIVFxKLiYk8j44dqcq7ocE+1vR0zVoknOx8IgtVO30V8jfAR4Cw2YGUjB3JlYsKgNHxLNfcNswTY0cZHc+yZcO55aICYGPfyvIf2dL4lh0jbOxbyUd37S3/rFzeh+98gPEjU1Vtpb7X3r6HSy84q1xUAAz0ryrvAMvrrBivnP+6nXu55nUvLxcVpenX7dzL+JGpcp9EkCgXFaU+W6O4ZxufKlD1fkfHs1x7+x5edXbPMTFde/seBvpXcd3OvWzZcG55W44dydXr1ycLzFg2d0zebd0xwlh29hw72flEFqq2KCzM7E3As+4+MkufzWY2bGbDBw4caEhcuXyhvLMpGR3PsiiVAGBppqNq+kzj03/OtLzKtlLf0L1qnkRgs65z+vzT+1eus9QnMGZcxmzj+dBrzleYob0US2k5o+NZcvkCp4Jm5KdUmykf86HXZb52ovyUOLVFYQFcArzFzPYBdwCXmtmOyg7ufrO797t7//LlyxsSVCqZoLc7U9XW253haK74x/Bgdqpq+kzj03/OtLzKtlLfwKxqnkLos65z+vzT+1eus9QndGZcxmzjycBqzpeYob0US2k5vd0ZUsnqoqpdNSM/pdpM+Zg8zldtJztfO1F+SpzaorBw9z9z9153Xw1cCXzf3QebHBY9XSluuaq/vNMpnWNxds8ierszbN/9ONsG1pan7xrZz9Cm9VXj2wf72DWyn+s3ri3/rFzeDVeso7uro6qt1PemTev5/kNPc1PFMncOP8nQYF/1OivGK+ffNrCWW+79FTdcsa5q+raBtXR3dZT7FMIC26ctYyiKe7bxjgRV77d0LsV9T4wdE9NNm9azc/hJtg2sZfvux8vbsnQyrMh89WRSx+Td0GAfPZnZc+xk5xNZqMy9vQ7nmdkG4E/d/U0z9env7/fh4eGGxDPXq0LyoZOsvCrEnYTNclWIOx1B7atCim0nf1VIYIAZAfW+KsTpTFqrXRXS9H8zG5mfUq0NrgpRftbZ6o99a859933i8jpG0pbmlJ9td0qzu+8Gdjc5jLIgMJYv6TymvVbbjLpiDCiydFH86+g+3jJmmj5D+0sytdvrsT1EANLpJCtOoiA42flEFqK2+CpERERE2oMKCxEREYmNCgsRERGJjQoLERERiY0KCxEREYmNCgsRERGJjQoLERERiY0KCxEREYmNCgsRERGJTVMLCzMLzOy0ZsYgIiIi8Wl4YWFmXzGz08ysC3gIeNjMrmt0HCIiIhK/ZhyxuMDdXwD+ELgbWAW8swlxiIiISMyaUVh0mFkHxcLiH9x9CmivR6yKiIhITc0oLD4H7KP4DMt7zexs4IUmxCEiIiIxa/hzgN3908CnK5qeMLPfbXQcIiIiEr+GFxZmthS4Clg9bf0faHQsIiIiEq+GFxYUT9j8EfAgEDZh/SIiIlInzSgs0u7+xycyg5mlgXuBToox73T3v6hHcI0Qhs7YkRy5fIGOZEBXB0zkYXIqxAIIQyiEThAYHYGRCIxcISTTEXA0F1IInURgmIE7pFMBk7mQqdBJBkYyMCbzIUE0bEBnh5HLO+4QOuQKIZ3JgEUp49BESD6aN5UMKIROR8LoXtRJEFizN5dIlYmJPGPZXDlnezIp0ulm7MpEpJZmfBq/bGbXAHcBk6VGd39+lnkmgUvd/XB0Rcm/mNk/uvuP6hxr7MLQefiZQ1xz2zCj41k+c+Va1q1axm+zeT7zvUe4+uJz+OiuvYyOZ+ntzrBtYC2nL06xOJ1k9OAkW3eMlKddv3Et9z78DG++sJctFe3bBtbyyW8/zIHDk2wbWMuiVIJFnUlekk7w3OEc79mxp9x3+2Afn/7eI3z3oWfp7c5w06b1dCaNiamQ3x7Ns/r0LhUX0jImJvI8Onak6nMwNNjHmp4uFRciLaIZV4XkgG3AvwEj0Wt4thm86HA02hG92vIS1bEjuXJRAfCqs3vI5Z2tO0bY2LeyXFQAjI5nuW7nXkbHJyiElHempWkf3bWXgf5V5aKicp4tG84tDz9/ZIrR57PkCvDsoVxV3y3Rekvj196+h0SQ4PkjUzzx/FHGjuQavYlEZjSWzR3zOdi6Y4SxrPJUpFU0o8T/Y+A/uPtzJzKTmSUoFiH/Afisu/942vTNwGaAVatWxRRq/HL5QnmnCNFXHlbcQS7NdFRNg2L7olSCQug1pyUCq9m+NNNRNX9pXaXhWn1L44FR7pfLF+b5jgXaJz9bXX6Gz0E+bMv/M1qG8lPi1IwjFj8Hjp7oTO5ecPcLgV7g1Wb2O9Om3+zu/e7ev3z58phCjV8qmaC3O1MeTwRG6NDbneFgdqpqGhTbj+YKJAKrOa0Qes32g9mpqvlLyziaK8zYtzQeOuV5UsnqQkROTrvkZ6tLzvA5SOrrunlRfkqcmlFYFID7zexzZvbp0muuM7v7QWA3cFm9Aqynnq4Ut1zVX9453vfEGKmkMTTYx66R/Vy/cW15Wul8id7uNIkAhgb7qqZdv3EtO4efZPu09m0Da9m++/Hy8LKuDnqXZUgl4Iwlqaq+26P1lsZv2rSeQlhgWVcHZy9bRE9XqtGbSGRGPZnUMZ+DocE+ejLKU5FWYe6NPYRoZlfXanf3jK4SAwAAGFZJREFUW2eZZzkw5e4HzSwDfBe43t3vqtW/v7/fh4dnPW2jqeZ9VYg7CTv2qpB8dLXIXK4KmSqEpJIBizqNQ9kFdVVI099Qq+dnqzvFrwpRftbZ6o99a859933i8jpG0pbmlJ/NuPPmrWaWAs6Lmh6Onhcym7OAW6PzLALgzpmKinYQBMbyJZ1VbV1znLd7po5zXUANp6VPfl6RRkunk6w4dQoJkVNOM+68uQG4leLzQgxYaWZXu/u9M83j7nuBVzUkQBERETlpzSj7PwW8wd0fBjCz84C/A/qaEIuIiIjEqCmPTS8VFQDu/gjF+1KIiIhIm2vGEYthM/sC8OVofBPF+1OIiIhIm2tGYbEVeC/Fp5kaxWeA3NSEOERERCRmzbgqZNLM/ha4h+JtuedyVYiIiIi0gba4KkRERETag64KERERkdjoqhARERGJja4KERERkdjoqhARERGJTUMLi+hZH19w90Hgrxu5bhEREam/hp5j4e4FYHn0EDIRERE5xTTjq5B9wL+a2TeAI6VGd9cRDBERkTbXjMLiN9ErAJY0Yf0iIiJSJ8248+Z/a/Q6RUREpDGacefN84A/BVZXrt/dL210LCIiIhKvZnwV8jVgO/B5oNCE9YuIiEidNKOwyLv70InMYGYrgduAlwIhcLO731iP4Crl8yEHjkySL4Qs7kxwNBeSD53FnQkmporD6WRA6JAPQwIzzMAdFqUCjuZCCu6kEgEGWAD5vDMVOonASCUCEgHlZSUDI50KmMi9ON6RMAoO4LgbnUkjOxUSupPuSHB6VydBYPXeFCI1TUzkGcvmyvnak0mRTs9ttzKfeUWkdTXsU2xmy6LBb5rZtcDXgcnSdHd/fpbZ88CfuPseM1sCjJjZPe7+UL3izedDHn7mEO/ZMcLb+3rZ8L+cydYdI1z88h4GX3M2196+h+WLO/nIZedz3c69jI5n6e3OcP3Gtdz78DO86cJetu4YKbffeOWFnJZO8u4vDZfbPvuOV5EIArZU9Bsa7OOu+0f53D/vK45vWs9dDzzFG9e+jH/c+xsuX7eCa2/fU+5/y1X9nH/mEhUX0nATE3keHTtSledDg32s6ek6boEwn3lFWtXqj33rhPrv+8TldYqkuRp5H4sRYBi4GrgO+GHUVmqfkbs/7e57ouFDwC+AFfUM9tnDk7wn2um9df2LRcI1r3t5+Q/7lg3nlosKgNHxLB/dtZeB/lXl/qX2D95xP6PjE1Vtzx+ZKhcVpbatO0YY6F/14vjtexjoX8X7vnIfA/2ryusuTb/mtmHGjuTquSlEahrL5o7J8607RhjLHj8f5zOviLS2hv1r4O7nAJhZ2t0nKqeZWXquyzGz1cCrgB9Pa98MbAZYtWrVPKOFqUJY3umF7uXhRGDl4aWZjvJwyeh4tqpPZfuiVKKqbVEqMeP8tZY303JzeZ2q0urizs9WkA+9Zj7mQ6/rvBK/UzE/pXma8XTTH86x7RhmthjYBXzI3V+onObuN7t7v7v3L1++fN5BdiQCerszAARm5eFC6OXhg9mp8nBJb3emqk9l+9FcdQFwNFeYcf5ay5tpualkdcEirSfu/GwFycBq5mNyDl/LzWdeid+pmJ/SPA0rLMzspWbWB2TM7FVmtj56bQAWzWH+DopFxe3u/vd1DpczFnfyucE+ersz/MOeUYai4Vvu/RU3bVpPb3eG7bsfZ9vA2vIOsnSOxc7hJ8v9S+03Xnkhvd3pqrZlXR1sn9ZvaLCPncNPvji+aT07h5/kb9/xKnYOP1led2n6LVf109OlO6RL4/VkUsfk+dBgHz2Z4+fjfOYVkdZm7o059GhmVwPvAvqpPqfiEPCl2YoFMzPgVuB5d//Q8dbV39/vw8OznrYxJw25KiRB1VUgpatCClGfjqRRCEFXhcSm6RsrrvxsBboqJHbKzzo7kRMsT/TkygVw8uac8rOR51jcCtxqZhvdfdcJzn4J8E7gQTO7P2r7z+5+d6xBTpNMBpz1khcP13Z3ndj8c+4/vd8JrkekWdLpJCtOshiYz7wi0rqacUvvXWZ2OfBKIF3R/pezzPMvtEAlLyIiIrNr+MmbZrYdeDvwforFwtuAsxsdh4iIiMSvGVeFXOzuVwHj0QPJXgOsbEIcIiIiErNmFBali9ePmtnLgCngnCbEISIiIjFrxplTd5nZUuCTFO+6CcUHkomIiEiba0Zh8f8BW4H/BPwb8M/ACT2UTERERFpTMwqLWyneu+LT0fgfUXxy6RVNiEVERERi1IzC4nx3X1cx/gMze6AJcYiIiEjMmlFY3GdmF7n7jwDM7D8C/9qEOERERNpCO93Vs2GFhZk9CDjQAVxlZk9G42cDDzUqDhERaV3t9AdUamvkEYs3NXBdIiIi0gSNfFbIE41al4iIiDRHM26QJSIiIqcoFRYiIiISGxUWIiIiEhsVFiIiIhIbFRYiIiISm7YoLMzsi2b2rJn9rNmxiIiIyMyacefNk/El4G8pPlOkIcLQOZjNMZkPSSUMM8jmQswAh3zoJIJie0ciIJcPyYdOOhkQOkwVQlLJgCCAqbwTBOAhLF/cSUdHolFvQ+S4JibyjGVz5EMnGRg9mRTpdLvsGkSk1bTF3sPd7zWz1Y1aXxg6+8aOcGgiz31PjPGG//Usxg5P8ZnvPcL//dqX8ydfe4DR8Sy93RmGNq0HYOvte1i+uJOPXHY+1+3cW57+uXf28Y37Rnnd+Wdy6w9/zftffx6vOGOxigtpCRMTeR4dO8LWHSMv5vRgH2t6ulRciMhJaYuvQhpt7EiOJ8aO8t6v7OHSC86iUICtO0bY2LeyXFQAjI5nee5wjq2372F0PMuWDeeWi4rS9Pd8eYSB/lV8dNdeNvatZOuOEZ49PNnMtydSNpbNlYsKKObs1h0jjGVzTY5MRNrVKVNYmNlmMxs2s+EDBw7Ma1m5fIFFqQSj41lCdwrujI5nWZrpKO+AS0r9gJrTR8ezJAKrmj8f+rzik/YTZ37GKR96zZxVji4srZqf0p5OmWOd7n4zcDNAf3//vPaKqWSCo7kCvd0ZAjMM6O3OcDA7RW93pmpHXOo3Op6tOb23O0Mh9Kr5k4HNJzxpQ3HmZ5ySgdXMWeXownKi+akHhclsTpkjFnHq6Upxds8iPvuO9Xz/oadJJGBosI9dI/v51NvW0dudAYo74NMXpxjatJ7e7gzbdz/OtoG1VdM/984+dg4/yfUb17JrZD9Dg32csbizmW9PpKwnk2JosK8qZ4cG++jJpJocmYi0q7Y4YmFmfwdsAE43s1HgL9z9C/VaXxAYq3u6OJjNccZpLyOVMFYs7eQv3vxKAoM7N19E3p2EGYFBMhHw1c0Xla8K+dp7XsNUGJJKFK8KuericwgC+PibX6mrQqSlpNNJ1vR0lfNXV4WIyHy1xd7D3f+o0esMAmNZ17QjC12NjkKk/tLpJCtUSIhITPRViIiIiMRGhYWIiIjERoWFiIiIxEaFhYiIiMRGhYWIiIjERoWFiIiIxEaFhYiIiMRGhYWIiIjERoWFiIiIxEaFhYiIiMRGhYWIiIjERoWFiIiIxEaFhYiIiMRGhYWIiIjERoWFiIiIxEaFhYiIiMRGhYWIiIjEJtnsAObKzC4DbgQSwOfd/RMns5yJiTx58hzNQa4QUgidjsDIpAKyUyH5gpPuSJAvhCQThjtMhU4hdFKJADOYzIekEgEJg2w+JBEYHYHRkTS6kkk6O9tms0qLmZjIM5bNkQ+dZGD0ZFKk03PLp/nMKyISl7bY65hZAvgs8HvAKPBTM/uGuz90IsuZmMiTI8+zL0xx4NAk1+3cy+h4lt7uDNsH+/jm/aP8ZN9BPnLZ+ez+5TNs7F/J2OFcVb8br7yQv7rrFxw4PMkNV6zjv9/9Sw4cnmTbwFpOX9LJRGdID6i4kBM2MZHn0bEjbN0xUs63ocE+1vR0HbdAmM+8IiJxapevQl4NPObuv3L3HHAH8NYTXchYNsehbMj+57PlYgFgdDzLlh0jDPSvYsuGc7lu514G+lfx1PjEMf0+eMf9bNlwLqPjWT585wPl4et27mX0+SyFAjx3NBfjW5eFYiybKxcGUMy3rTtGGMseP5/mM6+ISJzapbBYAeyvGB+N2srMbLOZDZvZ8IEDB2ouJB86+dBZlEqUd8DlBY5nSQTG0kxHeXimfkszHTWHF6UShF5ch0ilueZnrXybSz7NZ16RueSnyFy1S2FhNdqq9pjufrO797t7//Lly2suJBkYycA4mivQ252pmtbbnaEQOgezU+XhmfodzE7VHD6aKxBYcR0ileaan7XybS75NJ95ReaSnyJz1S6FxSiwsmK8F/jNiS6kJ5NiSSZg5bIM2wbWlnfEpXMsdg4/yfbdj7NtYC07h59kRXf6mH43Xnkh23c/Tm93hhuuWFce3jawlt5lGRIJOH1RKoa3LAtNTybF0GBfVb4NDfbRkzl+Ps1nXhGROLXLWV0/BdaY2TnAU8CVwDtOdCHpdBIm4KWnwWnpDu7YfFHVVSFXX3IOmy4qXhVy9rJFJBPGS6J+Yeh0RFeF3HjlhXREV4XceOWFBLoqRGKQTidZ09PFVzdfdMJXdsxnXhGROLXFXsfd82b2PuA7FC83/aK7//xkllXc0SZZnD52Wvd8ghSJQTqdZMVJFgPzmVdEJC5tsxdy97uBu5sdh4iIiMysXc6xEBERkTbQNkcsREREZG5Wf+xbJ9R/3ycuj23dOmIhIiIisVFhISIiIrEx91PvznxmdgB44jjdTgeea0A4J6uV42vn2J5z98saFUwtbZyfrRZTq8UD849J+Vl/7Rp7K8Q9p/w8JQuLuTCzYXfvb3YcM2nl+BRb/bXi+2i1mFotHmjNmOqhnd9nu8beTnHrqxARERGJjQoLERERic1CLixubnYAx9HK8Sm2+mvF99FqMbVaPNCaMdVDO7/Pdo29beJesOdYiIiISPwW8hELERERiZkKCxEREYnNgiwszOwyM3vYzB4zs481aJ0rzewHZvYLM/u5mX0wav+4mT1lZvdHrz+omOfPohgfNrPfr2f8ZrbPzB6MYhiO2paZ2T1m9mj0sztqNzP7dLT+vWa2vmI5V0f9HzWzq2OI6/yKbXO/mb1gZh9qle02X8eLycw6zeyr0fQfm9nqOsZSM0en9dlgZr+t2O5/Xq94KtZ5TG5Omz5jPtYhlpr5OK1Pw7dRI7Xi5+h45pLbrc7MEmZ2n5nd1exYjsvdF9SL4mPXHwdeDqSAB4ALGrDes4D10fAS4BHgAuDjwJ/W6H9BFFsncE4Uc6Je8QP7gNOntX0S+Fg0/DHg+mj4D4B/BAy4CPhx1L4M+FX0szsa7o75d/fvwNmtst3qnYvAtcD2aPhK4KuNztFpfTYAdzV4Ox2Tm9Om18zHBv3+/h04u9nbqIG/i5b7HM0x7uPmdqu/gD8GvtIOubUQj1i8GnjM3X/l7jngDuCt9V6puz/t7nui4UPAL4AVs8zyVuAOd590918Dj1GMvZHxvxW4NRq+FfjDivbbvOhHwFIzOwv4feAed3/e3ceBe4A47yL4euBxd5/troCtsN3mai4xVf4OdgKvNzOrRzAnkaOtYqZ8rLe55OOpphU/R8fVxrkNgJn1ApcDn292LHOxEAuLFcD+ivFRGpxg0eHsVwE/jpreFx3C/WLp6wZmjrNe8TvwXTMbMbPNUduZ7v40FD+YwBlNiq3kSuDvKsZbYbvNx1xiKvdx9zzwW6Cn3oHVyNFKrzGzB8zsH83slfWOhdq5WalZv9vp+Vip0duoUVrxc3RCjpPbrepvgI8AYbMDmYuFWFjU+m+vYdfcmtliYBfwIXd/ARgCzgUuBJ4GPlXqWmN2n6V9vi5x9/XAG4H3mtnrZunb6NgwsxTwFuBrUVOrbLf5mEtMDY+7Ro5W2kPx0P864DPA/6xnLJHj5WYzttH0fKzUjG3UKK34OZqz4+R2SzKzNwHPuvtIs2OZq4VYWIwCKyvGe4HfNGLFZtZBMalvd/e/B3D3Z9y94O4hcAvFQ42zxVmX+N39N9HPZ4GvR3E8UzqkHP18thmxRd4I7HH3Z6I4W2K7zdNcYir3MbMk8BLg+XoFVCtHK7n7C+5+OBq+G+gws9PrFU+0nlq5WakZv9uqfKzUjG3UQK34OZqT4+V2C7sEeIuZ7aP41dOlZrajuSHNbiEWFj8F1pjZOdF/HVcC36j3SqPvxb8A/MLd/7qivfK74P8T+Fk0/A3gyuiqgHOANcBP6hG/mXWZ2ZLSMPCGKI5vAKUrO64G/qEitquis/EvAn4bfVXyHeANZtYdfTXxhqgtDn9ExWHnVthuMZhLTJW/gwHg+x6dyRW3mXJ0Wp+Xls7xMLNXU9yHjNUjnmgdM+VmpZnysZ6q8rFSo7dRg7Xi5+i45pLbrcrd/8zde919NcXt/X13H2xyWLNr9tmjzXhRPIv8EYpnN/+XBq3ztRQPGe4F7o9efwB8GXgwav8GcFbFPP8livFh4I31ip/iGd4PRK+fl5ZJ8bv87wGPRj+XRe0GfDZa/4NAf8Wy/i+KJ0w+Brw7pm23iOKO+SUVbU3fbvXKReAvgbdEw2mKh9sfo1ggvbwJOboF2BL1eV+UIw8APwIurvP2mSk3K2OaMR/rFFOtfGzaNmqFnG3110y53ey4TuJ9bKANrgrRLb1FREQkNgvxqxARERGpExUWIiIiEhsVFiIiIhIbFRYiIiISGxUWIiIiEhsVFiIyKzNbbWbT7x1R93lFZnKieWVm7zKzl1WM7zuFblrWclRYnMLMLNHsGERqie4iKtIo7wJedrxOlZSjJ0+FRRszs/8ZPZjp56WHM5nZYTP7SzP7McUHIfWZ2T9F/b5TcYvua8zsp9GDknaZ2aKmvhlpdUkzuzV66NtOM1s0S271RXn1b8B7SwuI/mv8mpl9k+JDxczMtpnZz8zsQTN7e9RvpvYN0fruNLNHzOwTZrbJzH4S9Ts36ve2aN4HzOzexm8qaZBaOfnn0X7tZ2Z2c5RLA0A/cLuZ3W9mmWj+95vZnih3XgFgZh+P5vsucJuZpc3sf0R97jOz3436zdT+rmi//E0z+7WZvc/M/jjq8yMzWxb1+4CZPRTFfkfjN12dNfsOXXqd/IsX74SZoXib4x6Kd5e7ImrvAH4ILI/G3w58MRruqVjOXwHvb/b70as1X8DqKK8uica/CFw3S27tBf73aHgb8LNo+F0UnzVRytuNwD1AAjgTeBI4a5b2DcDBaLgTeAr4b9GyPgj8TTT8ILAiGl7a7O2nV8Ny8k9LuRW1fRl4czS8m+o7BO8r7fOAa4HPR8MfB0aATDT+J8D/iIZfEeViepb2d1G8S+4SYDnFpxGX7sh6A8WHn0Hx+Sqd0fApl6M6YtHePmBmpdsGr6T4XIwCxQftAJwP/A5wj5ndD/w/FB8aBPA7ZvbPZvYgsAk4lR7tLPHb7+7/Gg3vAH6fGrllZi+huKP8p6jvl6ct5x53Lz1E7bXA33nxYXLPAP8E/G+ztAP81N2fdvdJireU/m7U/iDFPzYA/wp8ycyuoVicyKlpek6+FvhdM/txtF+7lNn3a6UHkY3wYu4AfMPds9Hwa4ly2N1/CTwBnDdLO8AP3P2Qux+gWFh8M2qvzNG9FI+gDAL5E3jPbUHfIbUpM9sA/B/Aa9z9qJntplgxT7h7odQN+Lm7v6bGIr4E/KG7P2Bm76L436DITKbf+/8QNXLLzJbW6FvpSGX3GfrM1A4wWTEcVoyHRPszd99iZv8RuBy438wudPdT5SFg8qLpeebATRSPTOw3s49T3CfOpJQ7Bar/FtY9Rynm5uuAtwD/1cxe6e6nTIGhIxbt6yXAeFRUvAK4qEafh4HlZvYaKD422MxKFfwS4GkrPkp4U0Milna2qpRHFJ/s+SNq5Ja7HwR+a2avjfrOllv3Am83s4SZLae4o/3JLO1zYmbnuvuP3f3Pgeeofsy3nDqm5+S/RMPPmdliik8DLjlEcZ93ou4lymEzOw9YRXG/OlP7cZlZAKx09x8AHwGWAotPIraWpSMW7evbwBYz20sxoX80vYO756ITlz4dHaJOAn9D8cmL/xX4McVDeA9ych86WTh+AVxtZp+j+LTbzwDfoXZuvRv4opkdjfrM5OvAayg+BdSBj7j7v5vZTO2vmGOs28xsDcX/Kr8XLUdOPdNzcgjoprg/20fxEe8lXwK2m1mWYm7N1U3RfA9S/MriXe4+aWYztc9lmQlgR/S5MeCGqCA/ZejppiIiIhIbfRUiIiIisVFhISIiIrFRYSEiIiKxUWEhIiIisVFhISIiIrFRYSEiIiKxUWEhIiIisfn/AT8jDRy5p1/NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 540x540 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.pairplot(df[['area', 'bedrooms', 'bathrooms']]);\n",
    "# each of them has pretty strong positive relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.678</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4230.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Jan 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:09:39</td>     <th>  Log-Likelihood:    </th> <td> -84517.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6028</td>      <th>  AIC:               </th> <td>1.690e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6024</td>      <th>  BIC:               </th> <td>1.691e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td> 1.007e+04</td> <td> 1.04e+04</td> <td>    0.972</td> <td> 0.331</td> <td>-1.02e+04</td> <td> 3.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area</th>      <td>  345.9110</td> <td>    7.227</td> <td>   47.863</td> <td> 0.000</td> <td>  331.743</td> <td>  360.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>  <td>-2925.8063</td> <td> 1.03e+04</td> <td>   -0.285</td> <td> 0.775</td> <td> -2.3e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th> <td> 7345.3917</td> <td> 1.43e+04</td> <td>    0.515</td> <td> 0.607</td> <td>-2.06e+04</td> <td> 3.53e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>367.658</td> <th>  Durbin-Watson:     </th> <td>   2.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 350.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.536</td>  <th>  Prob(JB):          </th> <td>9.40e-77</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.503</td>  <th>  Cond. No.          </th> <td>1.16e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.16e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.678\n",
       "Model:                            OLS   Adj. R-squared:                  0.678\n",
       "Method:                 Least Squares   F-statistic:                     4230.\n",
       "Date:                Wed, 16 Jan 2019   Prob (F-statistic):               0.00\n",
       "Time:                        23:09:39   Log-Likelihood:                -84517.\n",
       "No. Observations:                6028   AIC:                         1.690e+05\n",
       "Df Residuals:                    6024   BIC:                         1.691e+05\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept   1.007e+04   1.04e+04      0.972      0.331   -1.02e+04    3.04e+04\n",
       "area         345.9110      7.227     47.863      0.000     331.743     360.079\n",
       "bedrooms   -2925.8063   1.03e+04     -0.285      0.775    -2.3e+04    1.72e+04\n",
       "bathrooms   7345.3917   1.43e+04      0.515      0.607   -2.06e+04    3.53e+04\n",
       "==============================================================================\n",
       "Omnibus:                      367.658   Durbin-Watson:                   2.007\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              350.116\n",
       "Skew:                           0.536   Prob(JB):                     9.40e-77\n",
       "Kurtosis:                       2.503   Cond. No.                     1.16e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.16e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intercept'] = 1\n",
    "lm = sm.OLS(df['price'], df[['intercept', 'area', 'bedrooms', 'bathrooms']])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The bedrooms has a negative coefficient associated with it.\n",
    "* Even though price and bedrooms have a positive relationship between one another, in the multiple linear regression model it showed up negative\n",
    "* The interpretation of this coefficient is now counter-intuitive to the relationship expected and what is actually true in the bivariate case.\n",
    "* This is one potential side effect of having multicollinearity in the model, is these flipped coefficients from what you expect to be true.\n",
    "* Another way to identify our predictors is correlated with one another, is **variance inflation factors** (VIFs)\n",
    "* It can be calculated for each x variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.327102</td>\n",
       "      <td>Intercept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.458190</td>\n",
       "      <td>area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.854484</td>\n",
       "      <td>bedrooms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.006851</td>\n",
       "      <td>bathrooms</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VIF Factor   features\n",
       "0    7.327102  Intercept\n",
       "1    5.458190       area\n",
       "2   20.854484   bedrooms\n",
       "3   19.006851  bathrooms"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,  X = dmatrices('price ~ area + bedrooms + bathrooms', df, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['features'] = X.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove at least one of these last two variables as both of their variants inflation factors are larger than 10.\n",
    "\n",
    "Vimos que quando vari√°veis ‚Äòx‚Äô est√£o relacionadas entre si, podemos inverter as rela√ß√µes em nossos modelos de regress√£o linear m√∫ltipla frente ao que esperar√≠amos quando olhamos para as rela√ß√µes bivariadas da regress√£o linear.\n",
    "\n",
    "Para saber mais sobre VIFs e multicolinearidade, aqui est√° a publica√ß√£o referenciada do v√≠deo sobre [VIFs](https://etav.github.io/python/vif_factor_python.html).\n",
    "\n",
    "* The case that X variables were correlated with one another can lead to flipped regression coefficients from the expected relationships and inaccurate hypothesis testing results.\n",
    "* When X variables are related to one another these results can be very misleading.\n",
    "* We saw two ways to identify multicollinearity: scatterplot matrix or variance inflation factors (VIFs).\n",
    "* If you have larger than ten for a VIF then you have multicollinearity in your model. \n",
    "* VIF for a particular variable is computed as one over one minus R squared, where the R squared is computed as the R squared for that X variable being predicted by all the other X variables.\n",
    "\n",
    "$VIF_i=\\frac{1}{1-R_i^2}$\n",
    "\n",
    "$x_i=b_0+b_1x_1+b_2x_2+...+b_n+x_n$\n",
    "\n",
    ">R2 = <1-R2\n",
    "\n",
    "It's unusual to find only one large VIF in a model, because if two or more X variables are related to one anoter you would expect each of these variables to have a high VIF.\n",
    "The most common way to work with variables that are corre\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
